import random
import os
import time
import json
import requests
import traceback
import re
import threading
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime, timedelta, timezone
from flask import Flask, jsonify

import undetected_chromedriver as uc
from fake_useragent import UserAgent

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- [1. ì„¤ì • ë° ë¡œê¹… ì´ˆê¸°í™”] ---
if not os.path.exists('logs'):
    os.makedirs('logs')

log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
file_handler = RotatingFileHandler('logs/crawler.log', maxBytes=10*1024*1024, backupCount=5)
file_handler.setFormatter(log_formatter)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

logger = logging.getLogger("PS-Collector")
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

app = Flask(__name__)
session = requests.Session()
session.headers.update({'Connection': 'keep-alive'})

# [í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ]
BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8080")
JAVA_API_URL = f"{BASE_URL}/api/v1/games/collect"
TARGET_API_URL = f"{BASE_URL}/api/v1/games/targets"
SELENIUM_URL = os.getenv("SELENIUM_URL")
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

lock = threading.Lock()
is_running = False

# --- [2. ì˜¤ë¼í´ í”„ë¦¬í‹°ì–´ ë§ì¶¤ ì„¤ì •] ---
CURRENT_MODE = os.getenv("CRAWLER_MODE", "LOW").upper()

CONFIG = {
    "LOW": {  # ğŸ¢ 1Core / 1GB RAM ìµœì í™”
        "restart_interval": 30,
        "page_load_strategy": "none",
        "sleep_min": 2.0,
        "sleep_max": 3.5,
        "timeout": 20,      # [ìˆ˜ì •] 10ì´ˆ -> 20ì´ˆ (CPU ë¶€í•˜ ëŒ€ë¹„)
        "window_stop": True
    },
    "HIGH": {
        "restart_interval": 100,
        "page_load_strategy": "normal",
        "sleep_min": 2.0,
        "sleep_max": 4.0,
        "timeout": 15,
        "window_stop": False
    }
}

CONF = CONFIG.get(CURRENT_MODE, CONFIG["LOW"])
logger.info(f"ğŸ”§ Crawler Config: {CURRENT_MODE} | Timeout: {CONF['timeout']}s")


# --- [3. í•µì‹¬ ê¸°ëŠ¥: ë“œë¼ì´ë²„ ë° ë°ì´í„° ì¶”ì¶œ] ---

def get_driver():
    """ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ìƒì„± (1280x720 í‘œì¤€ í•´ìƒë„ ì ìš©)"""
    ua = UserAgent()
    random_user_agent = ua.random

    window_size = "1280,720"

    logger.info(f"ğŸ­ UA: {random_user_agent} | ğŸ“ Size: {window_size}")

    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.popups": 2,
        "profile.default_content_setting_values.geolocation": 2,
        "disk-cache-size": 4096
    }

    driver = None

    if SELENIUM_URL:
        options = webdriver.ChromeOptions()
        options.page_load_strategy = CONF['page_load_strategy']
        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={window_size}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_experimental_option("prefs", prefs)

        driver = webdriver.Remote(command_executor=SELENIUM_URL, options=options)
    else:
        options = uc.ChromeOptions()
        options.page_load_strategy = CONF['page_load_strategy']

        if os.getenv("HEADLESS", "false").lower() == "true":
             options.add_argument("--headless=new")

        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={window_size}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        driver = uc.Chrome(options=options, use_subprocess=True)

    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    return driver

def clean_text(text):
    if not text: return ""
    text = re.sub(r'[â„¢Â®Â©â„ ]', '', text)
    text = text.replace('â€™', "'").replace('â€˜', "'").replace('â€œ', '"').replace('â€', '"')
    return re.sub(r'\s+', ' ', text).strip()

def get_json_from_browser(driver):
    """
    [ìˆ˜ì •] ì ìˆ˜ ê¸°ë°˜ ì¶”ì¶œ ë¡œì§ ì ìš©
    ë‹¨ìˆœ ê¸¸ì´ ë¹„êµê°€ ì•„ë‹ˆë¼, ê°€ê²© ì •ë³´(basePrice)ê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ìš°ì„  ì„ íƒí•©ë‹ˆë‹¤.
    """
    try:
        script_content = driver.execute_script("""
            const scripts = document.querySelectorAll('script[type="application/json"]');
            let bestContent = null;
            let maxScore = -1;

            for (const s of scripts) {
                const txt = s.textContent;

                // 1. ê¸°ë³¸ í•„í„°
                if (!txt.includes('apolloState') && !txt.includes('Product')) continue;

                // 2. ì ìˆ˜ ê³„ì‚°
                let score = 0;

                // ê¸¸ì´ ì ìˆ˜ (10ë§Œ ê¸€ìë‹¹ 1ì )
                score += (txt.length / 100000);

                // í•µì‹¬ ë°ì´í„° ê°€ì‚°ì 
                if (txt.includes('"__typename":"Product"') && txt.includes('"name":')) {
                    score += 100;
                }
                if (txt.includes('"webctas"') && txt.includes('"basePrice"')) {
                    score += 500; // ê°€ê²© ì •ë³´ê°€ ìˆìœ¼ë©´ ì••ë„ì  1ìˆœìœ„
                }

                if (score > maxScore) {
                    maxScore = score;
                    bestContent = txt;
                }
            }
            return bestContent;
        """)
        return script_content
    except Exception as e:
        logger.warning(f"   âš ï¸ JS Extraction Failed: {e}")
        return None

def parse_json_data(json_str, target_url):
    if not json_str: return None

    try:
        data = json.loads(json_str)
        cache = data.get("cache", {})
        product_data = None

        # 1. URL ID ë§¤ì¹­
        url_id_match = re.search(r'([A-Z]{4}\d{5}_00)', target_url)
        if url_id_match:
            target_id = url_id_match.group(1)
            for val in cache.values():
                if val.get("__typename") == "Product" and target_id in str(val.get("id", "")):
                    product_data = val
                    break

        # 2. ì •ë³´ëŸ‰(webctas) ê¸°ë°˜ ë§¤ì¹­
        if not product_data:
            for val in cache.values():
                if val.get("__typename") == "Product" and (val.get("webctas") or val.get("name")):
                    if not product_data or (len(val.get("webctas", [])) > len(product_data.get("webctas", []))):
                        product_data = val

        if not product_data: return None

        title = clean_text(product_data.get("name", ""))
        parsed_item = {
            "title": title,
            "englishTitle": clean_text(product_data.get("invariantName", "")),
            "publisher": clean_text(product_data.get("publisherName", "Unknown")),
            "platforms": product_data.get("platforms", []),
            "psStoreId": product_data.get("id", ""),
            "imageUrl": "",
            "description": "Full Data (JS-Extracted)",
            "genreIds": "",
            "originalPrice": 0, "currentPrice": 0, "discountRate": 0,
            "saleEndDate": None, "isPlusExclusive": False, "psPlusPrice": 0,
            "inCatalog": False
        }

        media_list = product_data.get("media", [])
        if not media_list:
             meta = product_data.get("personalizedMeta", {})
             media_list = meta.get("media", [])

        for media in media_list:
            if media.get("role") == "MASTER":
                parsed_item["imageUrl"] = media.get("url"); break
            if media.get("role") == "GAMEHUB_COVER_ART" and not parsed_item["imageUrl"]:
                parsed_item["imageUrl"] = media.get("url")

        genres = product_data.get("localizedGenres", [])
        parsed_item["genreIds"] = ", ".join([g.get("value") for g in genres])

        webctas = product_data.get("webctas", [])
        prices_found = []
        KST = timezone(timedelta(hours=9))

        for cta_ref in webctas:
            cta_key = cta_ref.get("__ref")
            if not cta_key: continue
            cta_obj = cache.get(cta_key)
            if not cta_obj: continue

            cta_type = cta_obj.get("type")

            if cta_type == "ADD_TO_LIBRARY":
                upsell = cta_obj.get("price", {}).get("upsellText", "")
                if "ì¹´íƒˆë¡œê·¸" in upsell or "PS_PLUS" in str(cta_obj):
                    parsed_item["inCatalog"] = True

            if cta_type in ["ADD_TO_CART", "PURCHASE", "PRE_ORDER"]:
                price_info = cta_obj.get("price", {})
                if price_info.get("isFree") is True and price_info.get("basePriceValue") == 0:
                    continue

                curr = price_info.get("discountedValue", 0)
                orig = price_info.get("basePriceValue", 0)
                is_plus = price_info.get("isExclusive", False)
                end_ts = price_info.get("endTime")

                end_date = None
                if end_ts:
                    try:
                        dt = datetime.fromtimestamp(int(end_ts)/1000, tz=timezone.utc).astimezone(KST)
                        end_date = dt.strftime('%Y-%m-%d')
                    except: pass

                if curr > 0:
                    prices_found.append({"curr": curr, "orig": orig, "is_plus": is_plus, "end_date": end_date})

        if prices_found:
            best_offer = min(prices_found, key=lambda x: x['curr'])
            parsed_item["currentPrice"] = best_offer['curr']
            parsed_item["originalPrice"] = best_offer['orig']
            parsed_item["saleEndDate"] = best_offer['end_date']
            parsed_item["isPlusExclusive"] = best_offer['is_plus']

            if parsed_item["originalPrice"] > parsed_item["currentPrice"]:
                parsed_item["discountRate"] = int(round(((parsed_item["originalPrice"] - parsed_item["currentPrice"]) / parsed_item["originalPrice"]) * 100))

        return parsed_item

    except Exception as e:
        logger.error(f"   âš ï¸ Python Parse Error: {e}")
        return None

def crawl_detail_and_send(driver, wait, target_url):
    try:
        driver.get(target_url)

        # 1ì°¨ ëŒ€ê¸°
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "script[type='application/json']")))
        except TimeoutException:
            logger.warning(f"   â³ Timeout (1st try): {target_url} - Retrying...")
            try:
                driver.refresh()
                time.sleep(3.0)
                if CONF["window_stop"]:
                    try: driver.execute_script("window.stop();")
                    except: pass
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "script[type='application/json']")))
            except TimeoutException:
                logger.error(f"   âŒ Timeout (Final): No JSON script found - {target_url}")
                return None

        if CONF["window_stop"]:
            try: driver.execute_script("window.stop();")
            except: pass

        json_str = get_json_from_browser(driver)

        if not json_str:
            logger.warning(f"   ğŸš« Empty Data (JS returned null): {target_url}")
            return None

        payload = parse_json_data(json_str, target_url)

        if not payload or not payload.get("title"):
            return None

        # [ìˆ˜ì •] 0ì› ë°ì´í„° ì „ì†¡ ë°©ì§€ ë¡œì§ (pass -> return None)
        if payload.get("currentPrice") == 0 and payload.get("originalPrice") == 0:
            logger.info(f"   ğŸš« Skip (0 Won): {payload['title']}")
            return None

        send_data_to_server(payload, payload["title"])
        return payload

    except Exception as e:
        logger.error(f"   ğŸ”¥ Error processing {target_url}: {e}")
        return None

def fetch_update_targets():
    try:
        res = session.get(TARGET_API_URL, timeout=10)
        if res.status_code == 200:
            targets = res.json()
            logger.info(f"ğŸ“¥ Received {len(targets)} targets.")
            return targets
    except Exception as e:
        logger.error(f"âŒ Connection Error: {e}")
    return []

def send_data_to_server(payload, title):
    try:
        res = session.post(JAVA_API_URL, json=payload, timeout=10)
        if res.status_code == 200:
            price_txt = f"{payload['currentPrice']:,}ì›"
            if payload.get("inCatalog"): price_txt += " [Catalog]"
            logger.info(f"   ğŸ“¤ Sent: {title} ({price_txt})")
        else:
            logger.error(f"   ğŸ’¥ Server Error {res.status_code}: {title}")
    except Exception as e:
        logger.error(f"   ğŸ’¥ Network Error: {e}")

def send_discord_summary(total_scanned, deals_list):
    if not DISCORD_WEBHOOK_URL: return
    try:
        total_deals = len(deals_list)
        if total_deals == 0: return

        sorted_deals = sorted(deals_list, key=lambda x: x.get('discountRate', 0), reverse=True)
        top_5 = sorted_deals[:5]

        message = f"## ğŸ“¢ [PS-Tracker] ì¼ì¼ ìˆ˜ì§‘ ë¦¬í¬íŠ¸ ({CURRENT_MODE})\n"
        message += f"**ğŸ—“ï¸ ë‚ ì§œ:** {datetime.now().strftime('%Y-%m-%d')}\n"
        message += f"**ğŸ“Š í†µê³„:** ì´ `{total_scanned}`ê°œ ìŠ¤ìº” / **`{total_deals}`**ê°œ í• ì¸ ê°ì§€!\n"
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"

        for i, game in enumerate(top_5, 1):
            sale_price = "{:,}".format(game.get('currentPrice', 0))
            message += f"{i}ï¸âƒ£ **[{game.get('discountRate', 0)}%] {game.get('title', 'Unknown')}**\n"
            message += f"ã€€ ğŸ’° **â‚©{sale_price}**\n"
            if i < len(top_5): message += "â”€â”€â”€\n"

        message += "\n[ğŸ”— ì‹¤ì‹œê°„ ìµœì €ê°€ í™•ì¸í•˜ê¸°](https://ps-signal.com)"
        requests.post(DISCORD_WEBHOOK_URL, json={"content": message})
        logger.info("ğŸ”” Discord Summary Report sent!")
    except Exception as e:
        logger.error(f"âŒ Failed to send Discord summary: {e}")

def run_batch_crawler_logic():
    global is_running
    logger.info(f"ğŸš€ [Crawler] Started. Mode: {CURRENT_MODE} (Standard Resolution)")

    driver = None
    total_processed_count = 0
    collected_deals = []

    try:
        driver = get_driver()
        wait = WebDriverWait(driver, CONF['timeout'])
        visited_urls = set()

        targets = fetch_update_targets()
        if targets:
            logger.info(f"Target Update: {len(targets)} games")
            for i, url in enumerate(targets):
                if not is_running: break

                if i > 0 and i % CONF["restart_interval"] == 0:
                    logger.info("â™»ï¸ Restarting driver (Memory Cleanup)...")
                    try: driver.quit()
                    except: pass
                    time.sleep(3)
                    driver = get_driver()
                    wait = WebDriverWait(driver, CONF['timeout'])

                res = crawl_detail_and_send(driver, wait, url)
                if res:
                    total_processed_count += 1
                    if res.get('discountRate', 0) > 0: collected_deals.append(res)
                visited_urls.add(url)

                time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

        if is_running:
            logger.info(f"ğŸ”­ [Phase 2] Starting Deep Discovery...")
            base_category_path = "https://store.playstation.com/ko-kr/category/3f772501-f6f8-49b7-abac-874a88ca4897"
            search_params = "?FULL_GAME=storeDisplayClassification&GAME_BUNDLE=storeDisplayClassification&PREMIUM_EDITION=storeDisplayClassification"

            current_page = 1
            max_pages = 15

            while current_page <= max_pages:
                if not is_running: break

                if current_page > 1 and current_page % 2 == 0:
                     logger.info("â™»ï¸ [Phase 2] Restarting driver...")
                     try: driver.quit()
                     except: pass
                     time.sleep(3)
                     driver = get_driver()
                     wait = WebDriverWait(driver, CONF['timeout'])

                target_list_url = f"{base_category_path}/{current_page}{search_params}"
                logger.info(f"   ğŸ“– Scanning Page {current_page}/{max_pages}")

                try:
                    driver.get(target_list_url)
                    try:
                        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='/product/']")))
                    except TimeoutException:
                        logger.warning(f"   âš ï¸ List page timeout. Retrying...")
                        driver.refresh()
                        time.sleep(3)

                    if CONF["window_stop"]:
                        try: driver.execute_script("window.stop();")
                        except: pass

                except Exception as e:
                    logger.warning(f"âš ï¸ Page Load Error on {current_page}: {e}")
                    current_page += 1
                    continue

                page_candidates = []
                try:
                    link_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/product/']")
                    for el in link_elements:
                        url = el.get_attribute("href")
                        if url and "/ko-kr/product/" in url and url not in visited_urls:
                            if url not in page_candidates: page_candidates.append(url)
                except: pass

                if not page_candidates:
                    logger.info(f"ğŸ›‘ No new games found on page {current_page}. Finishing Phase 2.")
                    break

                for url in page_candidates:
                    if not is_running: break

                    res = crawl_detail_and_send(driver, wait, url)
                    if res:
                        total_processed_count += 1
                        if res.get('discountRate', 0) > 0: collected_deals.append(res)
                    visited_urls.add(url)
                    time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

                current_page += 1

        send_discord_summary(total_processed_count, collected_deals)

    except Exception as e:
        logger.error(f"Critical Error: {e}")
        logger.error(traceback.format_exc())
    finally:
        if driver:
            try: driver.quit()
            except: pass
        with lock: is_running = False
        logger.info("ğŸ Crawler finished.")

@app.route('/run', methods=['POST'])
def trigger_crawl():
    global is_running
    with lock:
        if is_running: return jsonify({"status": "running"}), 409
        is_running = True
    thread = threading.Thread(target=run_batch_crawler_logic)
    thread.daemon = True
    thread.start()
    return jsonify({"status": "started"}), 200

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "UP", "running": is_running}), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)