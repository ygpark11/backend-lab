import random
import os
import time
import json
import requests
import traceback
import re
import threading
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime, timedelta, timezone
from flask import Flask, jsonify

import undetected_chromedriver as uc
from fake_useragent import UserAgent

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- [1. ì„¤ì • ë° ë¡œê¹… ì´ˆê¸°í™”] ---
if not os.path.exists('logs'):
    os.makedirs('logs')

log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
file_handler = RotatingFileHandler('logs/crawler.log', maxBytes=10*1024*1024, backupCount=5)
file_handler.setFormatter(log_formatter)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

logger = logging.getLogger("PS-Collector")
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

app = Flask(__name__)
session = requests.Session()
session.headers.update({'Connection': 'keep-alive'})

# [í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ]
BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8080")
JAVA_API_URL = f"{BASE_URL}/api/v1/games/collect"
TARGET_API_URL = f"{BASE_URL}/api/v1/games/targets"
SELENIUM_URL = os.getenv("SELENIUM_URL")
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

lock = threading.Lock()
is_running = False

# --- [2. ì˜¤ë¼í´ í”„ë¦¬í‹°ì–´ ë§ì¶¤ ì„¤ì •] ---
CURRENT_MODE = os.getenv("CRAWLER_MODE", "LOW").upper()

CONFIG = {
    "LOW": {  # ğŸ¢ 1Core / 1GB RAM ìµœì í™”
        "restart_interval": 30,         # 30ê°œë§ˆë‹¤ ì¬ì‹œì‘ (ë©”ëª¨ë¦¬ ë°©ì–´)
        "page_load_strategy": "none",   # HTML í—¤ë”ë§Œ ë°›ê³  ë©ˆì¶¤ (í•µì‹¬)
        "sleep_min": 1.5,
        "sleep_max": 2.5,
        "timeout": 10,
        "window_stop": True             # ê°•ì œ ë¡œë”© ì¤‘ë‹¨ í™œì„±í™”
    },
    "HIGH": { # ğŸï¸ ê³ ì‚¬ì–‘ í™˜ê²½
        "restart_interval": 100,
        "page_load_strategy": "normal",
        "sleep_min": 2.0,
        "sleep_max": 4.0,
        "timeout": 15,
        "window_stop": False
    }
}

CONF = CONFIG.get(CURRENT_MODE, CONFIG["LOW"])
logger.info(f"ğŸ”§ Crawler Config: {CURRENT_MODE} | Strategy: {CONF['page_load_strategy']}")


# --- [3. í•µì‹¬ ê¸°ëŠ¥: ë“œë¼ì´ë²„ ë° ë°ì´í„° ì¶”ì¶œ] ---

def get_driver():
    """ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ìƒì„± (1280x720 í‘œì¤€ í•´ìƒë„ ì ìš©)"""
    ua = UserAgent()
    random_user_agent = ua.random

    # [ì¤‘ìš”] ëª¨ë°”ì¼ ë·° ë°©ì§€ ë° ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•œ 'í‘œì¤€' í•´ìƒë„
    window_size = "1280,720"

    logger.info(f"ğŸ­ UA: {random_user_agent} | ğŸ“ Size: {window_size}")

    # ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ ì„¤ì • (ì´ë¯¸ì§€, í°íŠ¸, íŒì—… ë“±)
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.popups": 2,
        "profile.default_content_setting_values.geolocation": 2,
        "disk-cache-size": 4096
    }

    driver = None

    if SELENIUM_URL: # Docker/Grid í™˜ê²½
        options = webdriver.ChromeOptions()
        options.page_load_strategy = CONF['page_load_strategy']
        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={window_size}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_experimental_option("prefs", prefs)

        driver = webdriver.Remote(command_executor=SELENIUM_URL, options=options)
    else: # ë¡œì»¬ í™˜ê²½ (Undetected Chrome)
        options = uc.ChromeOptions()
        options.page_load_strategy = CONF['page_load_strategy']

        if os.getenv("HEADLESS", "false").lower() == "true":
             options.add_argument("--headless=new")

        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={window_size}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        driver = uc.Chrome(options=options, use_subprocess=True)

    # WebDriver ì†ì„± ìˆ¨ê¸°ê¸°
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    return driver

def clean_text(text):
    if not text: return ""
    text = re.sub(r'[â„¢Â®Â©â„ ]', '', text)
    text = text.replace('â€™', "'").replace('â€˜', "'").replace('â€œ', '"').replace('â€', '"')
    return re.sub(r'\s+', ' ', text).strip()

def get_json_from_browser(driver):
    """
    [í•µì‹¬ ì—…ê·¸ë ˆì´ë“œ]
    ë‹¨ìˆœ ê¸¸ì´ ë¹„êµê°€ ì•„ë‹ˆë¼, ë°ì´í„°ì˜ ì§ˆ(Quality)ì„ í‰ê°€í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.
    íŒŒì´ì¬ì˜ ê°€ì¤‘ì¹˜ ë¡œì§ì„ JSë¡œ ì´ì‹í•˜ì—¬ ë¸Œë¼ìš°ì € ë‚´ë¶€ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    try:
        script_content = driver.execute_script("""
            const scripts = document.querySelectorAll('script[type="application/json"]');
            let bestContent = null;
            let maxScore = -1;

            for (const s of scripts) {
                const txt = s.textContent;

                // 1. ê¸°ë³¸ í•„í„°: í•µì‹¬ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ íƒˆë½ (CPU ì ˆì•½)
                if (!txt.includes('apolloState') && !txt.includes('Product')) continue;

                // 2. ì ìˆ˜ ê³„ì‚° (ì œìë‹˜ì˜ íŒŒì´ì¬ ë¡œì§ì„ JSë¡œ êµ¬í˜„)
                let score = 0;

                // (1) ë°ì´í„° ì–‘ ì ìˆ˜ (ê¸¸ì´ ê°€ì‚°ì )
                // ë„ˆë¬´ ì§§ì€ ê±´ ë¬´ì‹œí•˜ê³ , ê¸¸ìˆ˜ë¡ ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬ (10ë§Œ ê¸€ìë‹¹ 1ì )
                score += (txt.length / 100000);

                // (2) í•µì‹¬ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬
                // ë‹¨ìˆœ string matchingì´ parseë³´ë‹¤ ë¹ ë¥´ë¯€ë¡œ includes ì‚¬ìš©

                // ì´ë¦„ ì •ë³´ê°€ ìˆëŠ”ê°€? (Product & name)
                if (txt.includes('"__typename":"Product"') && txt.includes('"name":')) {
                    score += 100; // ì´ë¦„ ìˆìœ¼ë©´ 100ì 
                }

                // ê°€ê²© ì •ë³´ê°€ ìˆëŠ”ê°€? (ê°€ì¥ ì¤‘ìš” â˜…â˜…â˜…)
                // webctasì™€ basePriceê°€ ê°™ì´ ìˆì–´ì•¼ ì§„ì§œ ê°€ê²© ì •ë³´ì„
                if (txt.includes('"webctas"') && txt.includes('"basePrice"')) {
                    score += 500; // ê°€ê²© ìˆìœ¼ë©´ 500ì  (ì••ë„ì  ìš°ì„ ìˆœìœ„)
                }

                // 3. ìµœê³  ì ìˆ˜ ê°±ì‹ 
                if (score > maxScore) {
                    maxScore = score;
                    bestContent = txt;
                }
            }
            return bestContent;
        """)
        return script_content
    except Exception as e:
        logger.warning(f"   âš ï¸ JS Extraction Failed: {e}")
        return None

def parse_json_data(json_str, target_url):
    """ì¶”ì¶œëœ JSON ë¬¸ìì—´ íŒŒì‹± ë° ë°ì´í„° ì •ì œ"""
    if not json_str: return None

    try:
        data = json.loads(json_str)
        cache = data.get("cache", {})
        product_data = None

        # --- Product ê°ì²´ ì°¾ê¸° ì „ëµ ---
        # 1. URL ID ë§¤ì¹­ ì‹œë„
        url_id_match = re.search(r'([A-Z]{4}\d{5}_00)', target_url)
        if url_id_match:
            target_id = url_id_match.group(1)
            for val in cache.values():
                if val.get("__typename") == "Product" and target_id in str(val.get("id", "")):
                    product_data = val
                    break

        # 2. ì‹¤íŒ¨ ì‹œ ê°€ì¥ ì •ë³´ê°€ ë§ì€(webctas ë³´ìœ ) Product íƒìƒ‰
        if not product_data:
            for val in cache.values():
                if val.get("__typename") == "Product" and (val.get("webctas") or val.get("name")):
                    # ê¸°ì¡´ ì°¾ì€ ê²ƒë³´ë‹¤ webctas(ê°€ê²©ì •ë³´)ê°€ ë” ë§ìœ¼ë©´ êµì²´
                    if not product_data or (len(val.get("webctas", [])) > len(product_data.get("webctas", []))):
                        product_data = val

        if not product_data: return None

        # --- ë°ì´í„° ë§¤í•‘ ---
        title = clean_text(product_data.get("name", ""))
        parsed_item = {
            "title": title,
            "englishTitle": clean_text(product_data.get("invariantName", "")),
            "publisher": clean_text(product_data.get("publisherName", "Unknown")),
            "platforms": product_data.get("platforms", []),
            "psStoreId": product_data.get("id", ""),
            "imageUrl": "",
            "description": "Full Data (JS-Extracted)",
            "genreIds": "",
            "originalPrice": 0, "currentPrice": 0, "discountRate": 0,
            "saleEndDate": None, "isPlusExclusive": False, "psPlusPrice": 0,
            "inCatalog": False
        }

        # [ì´ë¯¸ì§€] MASTER -> GAMEHUB_COVER_ART ìš°ì„ ìˆœìœ„
        media_list = product_data.get("media", [])
        if not media_list:
             meta = product_data.get("personalizedMeta", {})
             media_list = meta.get("media", [])

        for media in media_list:
            if media.get("role") == "MASTER":
                parsed_item["imageUrl"] = media.get("url"); break
            if media.get("role") == "GAMEHUB_COVER_ART" and not parsed_item["imageUrl"]:
                parsed_item["imageUrl"] = media.get("url")

        # [ì¥ë¥´]
        genres = product_data.get("localizedGenres", [])
        parsed_item["genreIds"] = ", ".join([g.get("value") for g in genres])

        # [ê°€ê²©] webctas ìˆœíšŒ
        webctas = product_data.get("webctas", [])
        prices_found = []
        KST = timezone(timedelta(hours=9))

        for cta_ref in webctas:
            cta_key = cta_ref.get("__ref")
            if not cta_key: continue
            cta_obj = cache.get(cta_key)
            if not cta_obj: continue

            cta_type = cta_obj.get("type")

            # ì¹´íƒˆë¡œê·¸ ì—¬ë¶€ í™•ì¸
            if cta_type == "ADD_TO_LIBRARY":
                upsell = cta_obj.get("price", {}).get("upsellText", "")
                if "ì¹´íƒˆë¡œê·¸" in upsell or "PS_PLUS" in str(cta_obj):
                    parsed_item["inCatalog"] = True

            # ê°€ê²© ì •ë³´ ì¶”ì¶œ
            if cta_type in ["ADD_TO_CART", "PURCHASE", "PRE_ORDER"]:
                price_info = cta_obj.get("price", {})
                if price_info.get("isFree") is True and price_info.get("basePriceValue") == 0:
                    continue

                curr = price_info.get("discountedValue", 0)
                orig = price_info.get("basePriceValue", 0)
                is_plus = price_info.get("isExclusive", False)
                end_ts = price_info.get("endTime")

                end_date = None
                if end_ts:
                    try:
                        dt = datetime.fromtimestamp(int(end_ts)/1000, tz=timezone.utc).astimezone(KST)
                        end_date = dt.strftime('%Y-%m-%d')
                    except: pass

                if curr > 0:
                    prices_found.append({"curr": curr, "orig": orig, "is_plus": is_plus, "end_date": end_date})

        # ìµœì €ê°€ ê²°ì •
        if prices_found:
            best_offer = min(prices_found, key=lambda x: x['curr'])
            parsed_item["currentPrice"] = best_offer['curr']
            parsed_item["originalPrice"] = best_offer['orig']
            parsed_item["saleEndDate"] = best_offer['end_date']
            parsed_item["isPlusExclusive"] = best_offer['is_plus']

            if parsed_item["originalPrice"] > parsed_item["currentPrice"]:
                parsed_item["discountRate"] = int(round(((parsed_item["originalPrice"] - parsed_item["currentPrice"]) / parsed_item["originalPrice"]) * 100))

        return parsed_item

    except Exception as e:
        logger.error(f"   âš ï¸ Python Parse Error: {e}")
        return None

def crawl_detail_and_send(driver, wait, target_url):
    try:
        # 1. í˜ì´ì§€ ì ‘ì† ('none' ì „ëµì´ë¼ HTML í—¤ë”ë§Œ ë°›ê³  ë¦¬í„´ë¨)
        driver.get(target_url)

        # 2. Script íƒœê·¸ ë¡œë”© ëŒ€ê¸° (UI ë Œë”ë§ ë¬´ì‹œ)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "script[type='application/json']")))
        except TimeoutException:
            logger.warning(f"   â³ Timeout: No JSON script found")
            return None

        # 3. ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ë¡œë”© ê°•ì œ ì¤‘ë‹¨ (CPU/ë©”ëª¨ë¦¬ ì ˆì•½)
        if CONF["window_stop"]:
            try: driver.execute_script("window.stop();")
            except: pass

        # 4. ë¸Œë¼ìš°ì € ë‚´ë¶€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        json_str = get_json_from_browser(driver)

        if not json_str:
            # ì‹¤íŒ¨ ì‹œ 1íšŒ ì¬ì‹œë„
            logger.info("   ğŸ”„ Retrying refresh...")
            driver.refresh()
            time.sleep(2)
            if CONF["window_stop"]:
                try: driver.execute_script("window.stop();")
                except: pass
            json_str = get_json_from_browser(driver)

        if not json_str:
            logger.warning(f"   ğŸš« Empty Data: {target_url}")
            return None

        # 5. íŒŒì´ì¬ íŒŒì‹± ë° ì „ì†¡
        payload = parse_json_data(json_str, target_url)

        if not payload or not payload.get("title"):
            return None

        # ë¬´ë£Œê±°ë‚˜ ê°€ê²© ì˜¤ë¥˜ì¸ ê²½ìš° íŒ¨ìŠ¤
        if payload.get("currentPrice") == 0 and payload.get("originalPrice") == 0:
            pass

        send_data_to_server(payload, payload["title"])
        return payload

    except Exception as e:
        logger.error(f"   ğŸ”¥ Error processing {target_url}: {e}")
        return None

def fetch_update_targets():
    try:
        res = session.get(TARGET_API_URL, timeout=10)
        if res.status_code == 200:
            targets = res.json()
            logger.info(f"ğŸ“¥ Received {len(targets)} targets.")
            return targets
    except Exception as e:
        logger.error(f"âŒ Connection Error: {e}")
    return []

def send_data_to_server(payload, title):
    try:
        res = session.post(JAVA_API_URL, json=payload, timeout=10)
        if res.status_code == 200:
            price_txt = f"{payload['currentPrice']:,}ì›"
            if payload.get("inCatalog"): price_txt += " [Catalog]"
            logger.info(f"   ğŸ“¤ Sent: {title} ({price_txt})")
        else:
            logger.error(f"   ğŸ’¥ Server Error {res.status_code}: {title}")
    except Exception as e:
        logger.error(f"   ğŸ’¥ Network Error: {e}")

def send_discord_summary(total_scanned, deals_list):
    """í¬ë¡¤ë§ ì¢…ë£Œ í›„ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ë””ìŠ¤ì½”ë“œë¡œ ì „ì†¡"""
    if not DISCORD_WEBHOOK_URL: return

    try:
        total_deals = len(deals_list)
        if total_deals == 0: return

        # í• ì¸ìœ¨ ë†’ì€ ìˆœ ì •ë ¬ ë° ìƒìœ„ 5ê°œ ì¶”ì¶œ
        sorted_deals = sorted(deals_list, key=lambda x: x.get('discountRate', 0), reverse=True)
        top_5 = sorted_deals[:5]

        message = f"## ğŸ“¢ [PS-Tracker] ì¼ì¼ ìˆ˜ì§‘ ë¦¬í¬íŠ¸ ({CURRENT_MODE})\n"
        message += f"**ğŸ—“ï¸ ë‚ ì§œ:** {datetime.now().strftime('%Y-%m-%d')}\n"
        message += f"**ğŸ“Š í†µê³„:** ì´ `{total_scanned}`ê°œ ìŠ¤ìº” / **`{total_deals}`**ê°œ í• ì¸ ê°ì§€!\n"
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"

        for i, game in enumerate(top_5, 1):
            sale_price = "{:,}".format(game.get('currentPrice', 0))
            message += f"{i}ï¸âƒ£ **[{game.get('discountRate', 0)}%] {game.get('title', 'Unknown')}**\n"
            message += f"ã€€ ğŸ’° **â‚©{sale_price}**\n"
            if i < len(top_5):
                message += "â”€â”€â”€\n"

        message += "\n[ğŸ”— ì‹¤ì‹œê°„ ìµœì €ê°€ í™•ì¸í•˜ê¸°](https://ps-signal.com)"
        requests.post(DISCORD_WEBHOOK_URL, json={"content": message})
        logger.info("ğŸ”” Discord Summary Report sent!")

    except Exception as e:
        logger.error(f"âŒ Failed to send Discord summary: {e}")

def run_batch_crawler_logic():
    global is_running
    logger.info(f"ğŸš€ [Crawler] Started. Mode: {CURRENT_MODE} (Standard Resolution)")

    driver = None
    total_processed_count = 0
    collected_deals = []

    try:
        driver = get_driver()
        wait = WebDriverWait(driver, CONF['timeout'])
        visited_urls = set()

        # [Phase 1] íƒ€ê²Ÿ ê°±ì‹  (ë³´ìœ  ë¦¬ìŠ¤íŠ¸)
        targets = fetch_update_targets()
        if targets:
            logger.info(f"Target Update: {len(targets)} games")
            for i, url in enumerate(targets):
                if not is_running: break

                # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„¤ì •ëœ ì£¼ê¸°ë§ˆë‹¤)
                if i > 0 and i % CONF["restart_interval"] == 0:
                    logger.info("â™»ï¸ Restarting driver (Memory Cleanup)...")
                    try: driver.quit()
                    except: pass
                    time.sleep(3)
                    driver = get_driver()
                    wait = WebDriverWait(driver, CONF['timeout'])

                res = crawl_detail_and_send(driver, wait, url)
                if res:
                    total_processed_count += 1
                    if res.get('discountRate', 0) > 0: collected_deals.append(res)
                visited_urls.add(url)

                # ì§§ì€ ëŒ€ê¸° (CPU ì¿¨ë‹¤ìš´)
                time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

        # [Phase 2] ì‹ ê·œ íƒìƒ‰ (í˜ì´ì§€ë„¤ì´ì…˜) - ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìŠ¤ìº”
        if is_running:
            logger.info(f"ğŸ”­ [Phase 2] Starting Deep Discovery...")
            base_category_path = "https://store.playstation.com/ko-kr/category/3f772501-f6f8-49b7-abac-874a88ca4897"
            search_params = "?FULL_GAME=storeDisplayClassification&GAME_BUNDLE=storeDisplayClassification&PREMIUM_EDITION=storeDisplayClassification"

            current_page = 1
            max_pages = 15

            while current_page <= max_pages:
                if not is_running: break

                # í˜ì´ì§€ ì´ë™ ì‹œì—ë„ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ë¦¬ìŠ¤íƒ€íŠ¸ ì ìš©
                if current_page > 1 and current_page % 2 == 0:
                     logger.info("â™»ï¸ [Phase 2] Restarting driver...")
                     try: driver.quit()
                     except: pass
                     time.sleep(3)
                     driver = get_driver()
                     wait = WebDriverWait(driver, CONF['timeout'])

                target_list_url = f"{base_category_path}/{current_page}{search_params}"
                logger.info(f"   ğŸ“– Scanning Page {current_page}/{max_pages}")

                try:
                    driver.get(target_list_url)
                    # 'none' ì „ëµì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ëŠ” ë§í¬ê°€ ë¡œë”©ë  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë‹¤ë ¤ì•¼ í•¨
                    try:
                        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='/product/']")))
                    except TimeoutException:
                        logger.warning(f"   âš ï¸ List page timeout. Retrying...")
                        driver.refresh()
                        time.sleep(3)

                    if CONF["window_stop"]:
                        try: driver.execute_script("window.stop();")
                        except: pass

                except Exception as e:
                    logger.warning(f"âš ï¸ Page Load Error on {current_page}: {e}")
                    current_page += 1
                    continue

                # ë§í¬ ìˆ˜ì§‘
                page_candidates = []
                try:
                    link_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/product/']")
                    for el in link_elements:
                        url = el.get_attribute("href")
                        if url and "/ko-kr/product/" in url and url not in visited_urls:
                            if url not in page_candidates: page_candidates.append(url)
                except: pass

                if not page_candidates:
                    logger.info(f"ğŸ›‘ No new games found on page {current_page}. Finishing Phase 2.")
                    break

                # ìˆ˜ì§‘ëœ ì‹ ê·œ í›„ë³´ë“¤ í¬ë¡¤ë§ (JS ì¶”ì¶œ ì—”ì§„ ì‚¬ìš©)
                for url in page_candidates:
                    if not is_running: break

                    res = crawl_detail_and_send(driver, wait, url)
                    if res:
                        total_processed_count += 1
                        if res.get('discountRate', 0) > 0: collected_deals.append(res)
                    visited_urls.add(url)
                    time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

                current_page += 1

        # ì¢…ë£Œ í›„ ë””ìŠ¤ì½”ë“œ ë¦¬í¬íŠ¸ ì „ì†¡
        send_discord_summary(total_processed_count, collected_deals)

    except Exception as e:
        logger.error(f"Critical Error: {e}")
        logger.error(traceback.format_exc())
    finally:
        if driver:
            try: driver.quit()
            except: pass
        with lock: is_running = False
        logger.info("ğŸ Crawler finished.")

# --- [Flask ì‹¤í–‰] ---
@app.route('/run', methods=['POST'])
def trigger_crawl():
    global is_running
    with lock:
        if is_running: return jsonify({"status": "running"}), 409
        is_running = True
    thread = threading.Thread(target=run_batch_crawler_logic)
    thread.daemon = True
    thread.start()
    return jsonify({"status": "started"}), 200

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "UP", "running": is_running}), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)