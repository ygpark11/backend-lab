import random
import os
import time
import json
import requests
import traceback
import re
import threading
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime, timedelta, timezone
from flask import Flask, jsonify

import undetected_chromedriver as uc

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- [1. ì„¤ì • ë° ë¡œê¹… ì´ˆê¸°í™”] ---
if not os.path.exists('logs'):
    os.makedirs('logs')

log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
file_handler = RotatingFileHandler('logs/crawler.log', maxBytes=10*1024*1024, backupCount=5)
file_handler.setFormatter(log_formatter)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

logger = logging.getLogger("PS-Collector")
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

app = Flask(__name__)
session = requests.Session()
session.headers.update({'Connection': 'keep-alive'})

# [í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ]
BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8080")
JAVA_API_URL = f"{BASE_URL}/api/v1/games/collect"
TARGET_API_URL = f"{BASE_URL}/api/v1/games/targets"
SELENIUM_URL = os.getenv("SELENIUM_URL")
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

lock = threading.Lock()
is_running = False

# --- [2. ì˜¤ë¼í´ í”„ë¦¬í‹°ì–´ ë§ì¶¤ ì„¤ì •] ---
CURRENT_MODE = os.getenv("CRAWLER_MODE", "LOW").upper()

CONFIG = {
    "LOW": {  # ğŸ¢ 1Core / 1GB RAM ìµœì í™”
        "restart_interval": 30,
        "page_load_strategy": "none",
        "sleep_min": 2.0,
        "sleep_max": 3.5,
        "timeout": 20,
        "window_stop": True
    },
    "HIGH": {
        "restart_interval": 100,
        "page_load_strategy": "normal",
        "sleep_min": 2.0,
        "sleep_max": 4.0,
        "timeout": 15,
        "window_stop": False
    }
}

CONF = CONFIG.get(CURRENT_MODE, CONFIG["LOW"])
logger.info(f"ğŸ”§ Crawler Config: {CURRENT_MODE} | Timeout: {CONF['timeout']}s")


# --- [3. í•µì‹¬ ê¸°ëŠ¥: ë“œë¼ì´ë²„ ë° ë°ì´í„° ì¶”ì¶œ] ---

def get_driver():
    """ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ìƒì„± (PC í™˜ê²½ ê³ ì • + 1920x1080 í•´ìƒë„ ì ìš©)"""

    DESKTOP_USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0"
    ]

    random_user_agent = random.choice(DESKTOP_USER_AGENTS)

    window_size = "1920,1080"

    logger.info(f"ğŸ­ UA: {random_user_agent} | ğŸ“ Size: {window_size}")

    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.popups": 2,
        "profile.default_content_setting_values.geolocation": 2,
        "disk-cache-size": 4096
    }

    driver = None

    if SELENIUM_URL:
        options = webdriver.ChromeOptions()
        options.page_load_strategy = CONF['page_load_strategy']
        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={window_size}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_experimental_option("prefs", prefs)

        driver = webdriver.Remote(command_executor=SELENIUM_URL, options=options)
    else:
        options = uc.ChromeOptions()
        options.page_load_strategy = CONF['page_load_strategy']

        if os.getenv("HEADLESS", "false").lower() == "true":
             options.add_argument("--headless=new")

        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={window_size}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        driver = uc.Chrome(options=options, use_subprocess=True)

    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    return driver

def clean_text(text):
    if not text: return ""
    text = re.sub(r'[â„¢Â®Â©â„ ]', '', text)
    text = text.replace('â€™', "'").replace('â€˜', "'").replace('â€œ', '"').replace('â€', '"')
    return re.sub(r'\s+', ' ', text).strip()

def get_json_from_browser(driver):
    """
    ì ìˆ˜ ê¸°ë°˜ ì¶”ì¶œ ë¡œì§ ì ìš©
    ë‹¨ìˆœ ê¸¸ì´ ë¹„êµê°€ ì•„ë‹ˆë¼, ê°€ê²© ì •ë³´(basePrice)ê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ìš°ì„  ì„ íƒí•©ë‹ˆë‹¤.
    """
    try:
        script_content = driver.execute_script("""
            const scripts = document.querySelectorAll('script[type="application/json"]');
            let bestContent = null;
            let maxScore = -1;

            for (const s of scripts) {
                const txt = s.textContent;

                // 1. ê¸°ë³¸ í•„í„°
                if (!txt.includes('apolloState') && !txt.includes('Product')) continue;

                // 2. ì ìˆ˜ ê³„ì‚°
                let score = 0;

                // ê¸¸ì´ ì ìˆ˜ (10ë§Œ ê¸€ìë‹¹ 1ì )
                score += (txt.length / 100000);

                // í•µì‹¬ ë°ì´í„° ê°€ì‚°ì 
                if (txt.includes('"__typename":"Product"') && txt.includes('"name":')) {
                    score += 100;
                }
                if (txt.includes('"webctas"') && txt.includes('"basePrice"')) {
                    score += 500; // ê°€ê²© ì •ë³´ê°€ ìˆìœ¼ë©´ ì••ë„ì  1ìˆœìœ„
                }

                if (score > maxScore) {
                    maxScore = score;
                    bestContent = txt;
                }
            }
            return bestContent;
        """)
        return script_content
    except Exception as e:
        logger.warning(f"   âš ï¸ JS Extraction Failed: {e}")
        return None

def parse_json_data(json_str, target_url):
    if not json_str: return None

    try:
        data = json.loads(json_str)
        cache = data.get("cache", {})
        product_data = None

        # 1. URL ID ë§¤ì¹­
        url_id_match = re.search(r'([A-Z]{4}\d{5}_00)', target_url)
        if url_id_match:
            target_id = url_id_match.group(1)
            for val in cache.values():
                if val.get("__typename") == "Product" and target_id in str(val.get("id", "")):
                    product_data = val
                    break

        # 2. ì •ë³´ëŸ‰(webctas) ê¸°ë°˜ ë§¤ì¹­
        if not product_data:
            for val in cache.values():
                if val.get("__typename") == "Product" and (val.get("webctas") or val.get("name")):
                    if not product_data or (len(val.get("webctas", [])) > len(product_data.get("webctas", []))):
                        product_data = val

        if not product_data: return None

        title = clean_text(product_data.get("name", ""))
        parsed_item = {
            "title": title,
            "englishTitle": clean_text(product_data.get("invariantName", "")),
            "publisher": clean_text(product_data.get("publisherName", "Unknown")),
            "platforms": product_data.get("platforms", []),
            "psStoreId": product_data.get("id", ""),
            "imageUrl": "",
            "description": "Full Data Crawler",
            "genreIds": "",
            "originalPrice": 0, "currentPrice": 0, "discountRate": 0,
            "saleEndDate": None, "isPlusExclusive": False, "psPlusPrice": 0,
            "inCatalog": False
        }

        media_list = product_data.get("media", [])
        if not media_list:
             meta = product_data.get("personalizedMeta", {})
             media_list = meta.get("media", [])

        for media in media_list:
            if media.get("role") == "MASTER":
                parsed_item["imageUrl"] = media.get("url"); break
            if media.get("role") == "GAMEHUB_COVER_ART" and not parsed_item["imageUrl"]:
                parsed_item["imageUrl"] = media.get("url")

        genres = product_data.get("localizedGenres", [])
        parsed_item["genreIds"] = ", ".join([g.get("value") for g in genres])

        webctas = product_data.get("webctas", [])
        prices_found = []
        KST = timezone(timedelta(hours=9))

        for cta_ref in webctas:
            cta_key = cta_ref.get("__ref")
            if not cta_key: continue
            cta_obj = cache.get(cta_key)
            if not cta_obj: continue

            cta_type = cta_obj.get("type")

            if cta_type == "ADD_TO_LIBRARY":
                upsell = cta_obj.get("price", {}).get("upsellText", "")
                if any(k in upsell for k in ["ì¹´íƒˆë¡œê·¸", "êµ¬ë…"]) or "PS_PLUS" in str(cta_obj):
                    parsed_item["inCatalog"] = True

            if cta_type in ["ADD_TO_CART", "PURCHASE", "PRE_ORDER"]:
                price_info = cta_obj.get("price", {})
                if price_info.get("isFree") is True and price_info.get("basePriceValue") == 0:
                    continue

                curr = price_info.get("discountedValue", 0)
                orig = price_info.get("basePriceValue", 0)
                is_plus = price_info.get("isExclusive", False)
                end_ts = price_info.get("endTime")

                end_date = None
                if end_ts:
                    try:
                        dt = datetime.fromtimestamp(int(end_ts)/1000, tz=timezone.utc).astimezone(KST)
                        end_date = dt.strftime('%Y-%m-%d')
                    except: pass

                if curr > 0:
                    prices_found.append({"curr": curr, "orig": orig, "is_plus": is_plus, "end_date": end_date})

        if prices_found:
            best_offer = min(prices_found, key=lambda x: x['curr'])
            parsed_item["currentPrice"] = best_offer['curr']
            parsed_item["originalPrice"] = best_offer['orig']
            parsed_item["saleEndDate"] = best_offer['end_date']
            parsed_item["isPlusExclusive"] = best_offer['is_plus']

            if parsed_item["originalPrice"] > parsed_item["currentPrice"]:
                parsed_item["discountRate"] = int(round(((parsed_item["originalPrice"] - parsed_item["currentPrice"]) / parsed_item["originalPrice"]) * 100))

        return parsed_item

    except Exception as e:
        logger.error(f"   âš ï¸ Python Parse Error: {e}")
        return None

def crawl_detail_and_send(driver, wait, target_url):
    try:
        driver.get(target_url)

        # 1ì°¨ ëŒ€ê¸°
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "script[type='application/json']")))
        except TimeoutException:
            logger.warning(f"   â³ Timeout (1st try): {target_url} - Retrying...")
            try:
                driver.refresh()
                time.sleep(3.0)
                if CONF["window_stop"]:
                    try: driver.execute_script("window.stop();")
                    except: pass
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "script[type='application/json']")))
            except TimeoutException:
                logger.error(f"   âŒ Timeout (Final): No JSON script found - {target_url}")
                return None

        if CONF["window_stop"]:
            try: driver.execute_script("window.stop();")
            except: pass

        json_str = get_json_from_browser(driver)

        if not json_str:
            logger.warning(f"   ğŸš« Empty Data (JS returned null): {target_url}")
            return None

        payload = parse_json_data(json_str, target_url)

        if not payload or not payload.get("title"):
            return None

        # 0ì› ë°ì´í„° ì „ì†¡ ë°©ì§€ ë¡œì§
        if payload.get("currentPrice") == 0 and payload.get("originalPrice") == 0:
            logger.info(f"   ğŸš« Skip (0 Won): {payload['title']}")
            return None

        send_data_to_server(payload, payload["title"])
        return payload

    except Exception as e:
        logger.error(f"   ğŸ”¥ Error processing {target_url}: {e}")
        return None

def fetch_update_targets():
    try:
        res = session.get(TARGET_API_URL, timeout=10)
        if res.status_code == 200:
            targets = res.json()
            logger.info(f"ğŸ“¥ Received {len(targets)} targets.")
            return targets
    except Exception as e:
        logger.error(f"âŒ Connection Error: {e}")
    return []

def send_data_to_server(payload, title):
    try:
        res = session.post(JAVA_API_URL, json=payload, timeout=10)
        if res.status_code == 200:
            price_txt = f"{payload['currentPrice']:,}ì›"
            if payload.get("inCatalog"): price_txt += " [Catalog]"
            logger.info(f"   ğŸ“¤ Sent: {title} ({price_txt})")
        else:
            logger.error(f"   ğŸ’¥ Server Error {res.status_code}: {title}")
    except Exception as e:
        logger.error(f"   ğŸ’¥ Network Error: {e}")

def send_discord_summary(total_scanned, deals_list):
    if not DISCORD_WEBHOOK_URL: return
    try:
        total_deals = len(deals_list)
        if total_deals == 0: return

        sorted_deals = sorted(deals_list, key=lambda x: x.get('discountRate', 0), reverse=True)
        top_5 = sorted_deals[:5]

        message = f"## ğŸ“¢ [PS-Tracker] ì¼ì¼ ìˆ˜ì§‘ ë¦¬í¬íŠ¸ ({CURRENT_MODE})\n"
        message += f"**ğŸ—“ï¸ ë‚ ì§œ:** {datetime.now().strftime('%Y-%m-%d')}\n"
        message += f"**ğŸ“Š í†µê³„:** ì´ `{total_scanned}`ê°œ ìŠ¤ìº” / **`{total_deals}`**ê°œ í• ì¸ ê°ì§€!\n"
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"

        for i, game in enumerate(top_5, 1):
            sale_price = "{:,}".format(game.get('currentPrice', 0))
            message += f"{i}ï¸âƒ£ **[{game.get('discountRate', 0)}%] {game.get('title', 'Unknown')}**\n"
            message += f"ã€€ ğŸ’° **â‚©{sale_price}**\n"
            if i < len(top_5): message += "â”€â”€â”€\n"

        message += "\n[ğŸ”— ì‹¤ì‹œê°„ ìµœì €ê°€ í™•ì¸í•˜ê¸°](https://ps-signal.com)"
        requests.post(DISCORD_WEBHOOK_URL, json={"content": message})
        logger.info("ğŸ”” Discord Summary Report sent!")
    except Exception as e:
        logger.error(f"âŒ Failed to send Discord summary: {e}")

def run_batch_crawler_logic():
    global is_running
    logger.info(f"ğŸš€ [Crawler] Started. Mode: {CURRENT_MODE} (Standard Resolution)")

    driver = None
    total_processed_count = 0
    collected_deals = []

    try:
        driver = None
        for try_cnt in range(1, 4):
            try:
                driver = get_driver()
                logger.info("âœ… Driver started successfully.")
                break
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to start driver (Attempt {try_cnt}/3): {e}")
                time.sleep(10) # 10ì´ˆ ìˆ¨ ê³ ë¥´ê¸° í›„ ì¬ì‹œë„

        if not driver:
            logger.error("âŒ Critical: Failed to start driver after 3 attempts.")
            return # ì™„ì „ ì¢…ë£Œ
        wait = WebDriverWait(driver, CONF['timeout'])
        visited_urls = set()

        # --- [Phase 1: ê¸°ì¡´ íƒ€ê²Ÿ ê°±ì‹ ] ---
        targets = fetch_update_targets()
        if targets:
            logger.info(f"Target Update: {len(targets)} games")
            for i, url in enumerate(targets):
                if not is_running: break

                if i > 0 and i % CONF["restart_interval"] == 0:
                    logger.info("â™»ï¸ [Phase 1] Restarting driver (Memory Cleanup)...")
                    try: driver.quit()
                    except: pass
                    time.sleep(3)
                    driver = get_driver()
                    wait = WebDriverWait(driver, CONF['timeout'])

                res = crawl_detail_and_send(driver, wait, url)
                if res:
                    total_processed_count += 1
                    if res.get('discountRate', 0) > 0: collected_deals.append(res)
                visited_urls.add(url)

                time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

        # --- [Phase 2: ì‹ ê·œ ë°œêµ´ (Deep Discovery)] ---
        if is_running:
            logger.info(f"ğŸ”­ [Phase 2] Starting Deep Discovery...")
            base_category_path = "https://store.playstation.com/ko-kr/category/3f772501-f6f8-49b7-abac-874a88ca4897"
            search_params = "?FULL_GAME=storeDisplayClassification&GAME_BUNDLE=storeDisplayClassification&PREMIUM_EDITION=storeDisplayClassification"

            current_page = 1
            max_pages = 15

            while current_page <= max_pages:
                if not is_running: break

                # [ë©”ëª¨ë¦¬ ê´€ë¦¬] 2í˜ì´ì§€ë§ˆë‹¤ ë¸Œë¼ìš°ì € ì¬ì‹œì‘
                if current_page > 1 and current_page % 2 == 0:
                     logger.info("â™»ï¸ [Phase 2] Restarting driver (Memory Cleanup)...")
                     try: driver.quit()
                     except: pass
                     time.sleep(5)
                     driver = get_driver()
                     wait = WebDriverWait(driver, CONF['timeout'])

                target_list_url = f"{base_category_path}/{current_page}{search_params}"
                logger.info(f"   ğŸ“– Scanning Page {current_page}/{max_pages}")

                raw_game_count = 0
                page_candidates = []
                page_load_success = False

                # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë¡œë”© (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)
                for try_cnt in range(1, 4):
                    try:
                        driver.get(target_list_url)

                        # DOM ê·¸ë¦¬ëŠ” ì‹œê°„ì´ í•„ìš”í•˜ë¯€ë¡œ ëŒ€ê¸°
                        time.sleep(3)

                        # ì˜¤ë¼í´ í”„ë¦¬í‹°ì–´ëŠ” í™”ë©´ ë Œë”ë§ì´ ëŠë¦¼ -> ê°•ì œ ìŠ¤í¬ë¡¤ë¡œ ë¡œë”© ìœ ë°œ
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight / 2);")
                        time.sleep(1)
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

                        # ëª…ì‹œì  ëŒ€ê¸°: ê²Œì„ ë§í¬ê°€ ëœ° ë•Œê¹Œì§€
                        WebDriverWait(driver, 30).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='/product/']"))
                        )

                        # ìš”ì†Œ ì¶”ì¶œ ì‹œë„
                        link_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/product/']")
                        if len(link_elements) > 0:
                            page_load_success = True

                            for el in link_elements:
                                url = el.get_attribute("href")
                                if url and "/ko-kr/product/" in url and url not in visited_urls:
                                    if url not in page_candidates: page_candidates.append(url)

                            raw_game_count = len(link_elements)
                            break # ì„±ê³µí–ˆìœ¼ë‹ˆ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
                        else:
                            raise Exception("Elements list is empty")

                    except Exception as e:
                        logger.warning(f"   âš ï¸ List load failed (Attempt {try_cnt}/3). Retrying... Error: {str(e)[:50]}")
                        # ì‹¤íŒ¨ ì‹œ ìƒˆë¡œê³ ì¹¨ ëŒ€ì‹  ì ì‹œ ëŒ€ê¸°
                        time.sleep(5)

                # 3ë²ˆ ì‹œë„í–ˆëŠ”ë°ë„ ì‹¤íŒ¨í–ˆê±°ë‚˜, ê²Œì„ì´ 0ê°œì¸ ê²½ìš°
                if not page_load_success:
                    logger.error(f"   âŒ Failed to load page {current_page} properly. Skipping...")
                    current_page += 1
                    continue

                # ë¡œë”©ì€ ì„±ê³µí–ˆëŠ”ë° ì§„ì§œ 0ê°œë©´ -> ì´ê±´ ì§„ì§œ ëë‚œ ê²ƒ
                # ë‹¨, í˜¹ì‹œ ëª¨ë¥´ë‹ˆ "í˜ì´ì§€ ë¡œë”© ì„±ê³µ" í”Œë˜ê·¸ê°€ ìˆëŠ”ë° 0ê°œì¸ ê²½ìš°ë§Œ ì¢…ë£Œ
                if raw_game_count == 0:
                    logger.info(f"ğŸ›‘ No games found on HTML. Assuming end of list at page {current_page}.")
                    break

                logger.info(f"      âœ… Found {len(page_candidates)} new items on page {current_page}.")

                # ì¶”ì¶œëœ ê²Œì„ë“¤ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘
                for url in page_candidates:
                    if not is_running: break

                    # ìƒì„¸ í˜ì´ì§€ëŠ” ë¡œì§
                    res = crawl_detail_and_send(driver, wait, url)
                    if res:
                        total_processed_count += 1
                        if res.get('discountRate', 0) > 0: collected_deals.append(res)
                    visited_urls.add(url)

                    time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

                current_page += 1

        send_discord_summary(total_processed_count, collected_deals)

    except Exception as e:
        logger.error(f"Critical Error: {e}")
        logger.error(traceback.format_exc())
    finally:
        if driver:
            try: driver.quit()
            except: pass
        with lock: is_running = False
        logger.info("ğŸ Crawler finished.")

@app.route('/run', methods=['POST'])
def trigger_crawl():
    global is_running
    with lock:
        if is_running: return jsonify({"status": "running"}), 409
        is_running = True
    thread = threading.Thread(target=run_batch_crawler_logic)
    thread.daemon = True
    thread.start()
    return jsonify({"status": "started"}), 200

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "UP", "running": is_running}), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)