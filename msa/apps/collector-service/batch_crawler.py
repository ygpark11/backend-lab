import random
import os
import time
import json
import requests
import traceback
import re
import threading
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime
from flask import Flask, jsonify

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- [ì„¤ì • ë° ë¡œê¹… ì´ˆê¸°í™”] ---
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists('logs'):
    os.makedirs('logs')

# ë¡œê¹… ì„¤ì • (ì½˜ì†” + íŒŒì¼ íšŒì „)
log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# 1. íŒŒì¼ í•¸ë“¤ëŸ¬ (10MB ì”© 5ê°œ ë³´ê´€)
file_handler = RotatingFileHandler('logs/crawler.log', maxBytes=10*1024*1024, backupCount=5)
file_handler.setFormatter(log_formatter)

# 2. ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

logger = logging.getLogger("PS-Collector")
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

app = Flask(__name__)

# í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬ (ê¸°ë³¸ê°’ ì„¤ì • ê°•í™”)
BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8080")
JAVA_API_URL = f"{BASE_URL}/api/v1/games/collect"
TARGET_API_URL = f"{BASE_URL}/api/v1/games/targets"
SELENIUM_URL = os.getenv("SELENIUM_URL")

# ë™ì‹œ ì‹¤í–‰ ë°©ì§€ ë½ (Lock)
lock = threading.Lock()
is_running = False

def get_driver():
    """ë“œë¼ì´ë²„ ì„¤ì • ë° ìƒì„± ë¡œì§ ë¶„ë¦¬"""
    options = webdriver.ChromeOptions()
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ko-KR")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")

    # User-Agent ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

    # Docker í™˜ê²½ì´ê±°ë‚˜ ëª…ì‹œì  í—¤ë“œë¦¬ìŠ¤ ìš”ì²­ ì‹œ
    if SELENIUM_URL or os.getenv("HEADLESS", "true").lower() == "true":
        options.add_argument("--headless")

    if SELENIUM_URL:
        logger.info(f"ğŸŒ [Docker Mode] Connecting to Selenium Grid: {SELENIUM_URL}")
        return webdriver.Remote(command_executor=SELENIUM_URL, options=options)
    else:
        logger.info("ğŸ’» [Local Mode] Starting Chrome Driver")
        return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def fetch_update_targets():
    """Java ì„œë²„ í†µì‹  ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”"""
    try:
        res = requests.get(TARGET_API_URL, timeout=10) # íƒ€ì„ì•„ì›ƒ ì¶”ê°€
        if res.status_code == 200:
            targets = res.json()
            logger.info(f"ğŸ“¥ Received {len(targets)} targets from Java Server.")
            return targets
        logger.warning(f"âš ï¸ Failed to fetch targets. Status: {res.status_code}")
        return []
    except Exception as e:
        logger.error(f"âŒ Connection Error to Java Server: {e}")
        return []

def run_batch_crawler_logic():
    global is_running
    logger.info("ğŸš€ [Crawler] Batch job started - Pagination Mode On")

    driver = None
    try:
        driver = get_driver()
        wait = WebDriverWait(driver, 15)
        visited_urls = set()

        # [Phase 1] ê¸°ì¡´ íƒ€ê²Ÿ ê°±ì‹ 
        targets = fetch_update_targets()
        if targets:
            logger.info(f"ğŸ”„ [Phase 1] Updating {len(targets)} tracked games...")
            for url in targets:
                if not is_running: break
                crawl_detail_and_send(driver, wait, url)
                visited_urls.add(url)
                time.sleep(random.uniform(1.0, 2.0))

        # [Phase 2] ì‹ ê·œ íƒìƒ‰ (í˜ì´ì§€ë„¤ì´ì…˜)
        if is_running:
            logger.info(f"ğŸ”­ [Phase 2] Starting Deep Discovery (Max 300 Pages)...")
            base_category_path = "https://store.playstation.com/ko-kr/category/3f772501-f6f8-49b7-abac-874a88ca4897"
            search_params = "?FULL_GAME=storeDisplayClassification&GAME_BUNDLE=storeDisplayClassification&PREMIUM_EDITION=storeDisplayClassification"

            current_page = 1
            max_pages = 300

            while current_page <= max_pages:
                if not is_running: break

                # [ë©”ëª¨ë¦¬ ê´€ë¦¬] 20í˜ì´ì§€ë§ˆë‹¤ ë“œë¼ì´ë²„ ì¬ì‹œì‘
                if current_page > 1 and current_page % 20 == 0:
                    logger.info("â™»ï¸ [Maintenance] Restarting driver to prevent memory leak...")
                    driver.quit()
                    time.sleep(5)
                    driver = get_driver()
                    wait = WebDriverWait(driver, 15)

                target_list_url = f"{base_category_path}/{current_page}{search_params}"
                logger.info(f"   ğŸ“– Scanning Page {current_page}/{max_pages}")

                try:
                    driver.get(target_list_url)
                    # ìŠ¤í¬ë¡¤ ë¡œì§
                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
                    driver.execute_script("window.scrollTo(0, 1000);")
                    time.sleep(1.5)
                    driver.execute_script("window.scrollTo(0, 4000);")
                    time.sleep(1.5)
                except Exception as e:
                    logger.warning(f"âš ï¸ Page Load Error on {current_page}: {e}")

                # ë§í¬ ìˆ˜ì§‘
                page_candidates = []
                try:
                    link_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/product/']")
                    for el in link_elements:
                        url = el.get_attribute("href")
                        if url and "/ko-kr/product/" in url and url not in visited_urls:
                            if url not in page_candidates:
                                page_candidates.append(url)
                except: pass

                # ì¢…ë£Œ ì¡°ê±´
                if not page_candidates:
                    logger.info(f"ğŸ›‘ No new games found on page {current_page}. Finishing Phase 2.")
                    break

                logger.info(f"      Found {len(page_candidates)} new candidates.")

                # ìƒì„¸ í¬ë¡¤ë§
                for url in page_candidates:
                    if not is_running: break
                    crawl_detail_and_send(driver, wait, url)
                    visited_urls.add(url)
                    time.sleep(random.uniform(1.0, 3.0))

                current_page += 1
                time.sleep(random.uniform(2.0, 3.0))

        logger.info(f"âœ… Batch job finished. Total processed: {len(visited_urls)} games.")

    except Exception as e:
        logger.error(f"ğŸ”¥ Critical Crawler Error: {e}")
        logger.error(traceback.format_exc())
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("ğŸ”Œ Driver closed.")
            except: pass

        with lock:
            is_running = False

def crawl_detail_and_send(driver, wait, target_url):
    try:
        driver.get(target_url)

        # 1. ì œëª© ë¡œë”©
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "[data-qa='mfe-game-title#name']")))
        except TimeoutException:
            logger.warning(f"â³ Timeout loading title: {target_url}")
            return

        # 2. ê°€ê²© ì»¨í…Œì´ë„ˆ ëŒ€ê¸° (ì—†ìœ¼ë©´ ë¬´ë£Œ ê²Œì„ì´ê±°ë‚˜ ë¡œë”© ì‹¤íŒ¨)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "[data-qa^='mfeCtaMain#offer']")))
        except:
            # ê°€ê²©ì´ ì—†ëŠ” ê²½ìš°(ì˜ˆ: ì¶œì‹œ ì˜ˆì •ì‘)ë„ ìˆìœ¼ë¯€ë¡œ ë¡œê·¸ë§Œ ì°ê³  ì§„í–‰
            logger.info("   â„¹ï¸ No price container found (Might be free or unreleased)")
            pass

        # 3. ì œëª© ì¶”ì¶œ
        try:
            title = driver.find_element(By.CSS_SELECTOR, "[data-qa='mfe-game-title#name']").text.strip()
            logger.info(f"   ğŸ“– Title: {title}")
        except:
            title = "Unknown Title"
            logger.warning("   âš ï¸ Failed to extract title")

        # 4. ì¥ë¥´ ì¶”ì¶œ
        genre_ids = ""
        try:
            genre_element = driver.find_element(By.CSS_SELECTOR, "[data-qa='gameInfo#releaseInformation#genre-value']")
            genre_ids = genre_element.text
        except: pass

        # 5. í”Œë«í¼ ì¶”ì¶œ
        platform_set = set()
        try:
            tag_elements = driver.find_elements(By.CSS_SELECTOR, "[data-qa^='mfe-game-title#productTag']")
            for el in tag_elements:
                raw_text = el.get_attribute("textContent").strip().upper()

                if "PS5" in raw_text: platform_set.add("PS5")
                if "PS4" in raw_text: platform_set.add("PS4")

                if "VR2" in raw_text:
                    platform_set.add("PS_VR2")
                elif "VR" in raw_text:
                    platform_set.add("PS_VR")

            platforms = list(platform_set)
            logger.info(f"   ğŸ® Platforms: {platforms}")
        except Exception as e:
            logger.warning(f"   âš ï¸ Platform parsing error: {e}")
            platforms = []

        # 6.. ê°€ê²© ì¶”ì¶œ
        best_price = float('inf')
        best_offer_data = None    # ìµœì €ê°€ì¼ ë•Œì˜ ì„¸ë¶€ ì •ë³´(ì›ê°€, í• ì¸ìœ¨, Plusì—¬ë¶€ ë“±)
        found_valid_offer = False

        # ìµœëŒ€ 2ë²ˆ ì‹œë„ (DOM ë Œë”ë§ ì§€ì—° ëŒ€ë¹„)
        for attempt in range(2):
            if found_valid_offer: break
            if attempt > 0: time.sleep(1.5)

            # ëª¨ë“  ì˜¤í¼(offer0 ~ offer2)ë¥¼ ë‹¤ í™•ì¸í•´ì„œ ê°€ì¥ ì‹¼ ê°€ê²©ì„ ì„ íƒ
            for i in range(3):
                try:
                    # í•´ë‹¹ ìˆœë²ˆ(i)ì˜ ê°€ê²© ë°•ìŠ¤ ì „ì²´ë¥¼ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
                    offer_selector = f"[data-qa='mfeCtaMain#offer{i}']"
                    try:
                        offer_container = driver.find_element(By.CSS_SELECTOR, offer_selector)
                    except:
                        continue # í•´ë‹¹ ë²ˆí˜¸ì˜ ì˜¤í¼ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒìœ¼ë¡œ

                    # 6-1. ê°€ê²© íŒŒì‹± (textContent ì‚¬ìš©ìœ¼ë¡œ í™”ë©´ ê°€ë¦¼ ë¬¸ì œ í•´ê²°)
                    try:
                        price_selector = f"[data-qa='mfeCtaMain#offer{i}#finalPrice']"
                        price_elem = offer_container.find_element(By.CSS_SELECTOR, price_selector)

                        # execute_script ëŒ€ì‹  get_attribute("textContent") ì‚¬ìš© (ê°€ì¥ ì•ˆì „í•¨)
                        raw_price = price_elem.get_attribute("textContent").strip()
                        clean_price_text = re.sub(r'[^0-9]', '', raw_price)

                        if not clean_price_text: continue
                        current_price = int(clean_price_text)
                        if current_price == 0: continue
                    except:
                        # ê°€ê²© íƒœê·¸ê°€ ì—†ìœ¼ë©´ ë¬´íš¨
                        continue

                    # 6-2. PS Plus ì—¬ë¶€ íŒŒì‹±
                    is_plus = False

                    # [Check 1] ë…¸ë€ìƒ‰ í…ìŠ¤íŠ¸ í´ë˜ìŠ¤ (ê°€ì¥ í™•ì‹¤)
                    # HTML: <span class="psw-c-t-ps-plus ...">PlayStation Plusë¡œ ...</span>
                    try:
                        if offer_container.find_elements(By.CSS_SELECTOR, ".psw-c-t-ps-plus"):
                            is_plus = True
                    except: pass

                    # [Check 2] ì•„ì´ì½˜ (serviceIcon#ps-plus)
                    # HTML: <span data-qa="mfeCtaMain#offer0#serviceIcon#ps-plus" ...>
                    if not is_plus:
                        try:
                            if offer_container.find_elements(By.CSS_SELECTOR, "[data-qa*='serviceIcon#ps-plus']"):
                                is_plus = True
                        except: pass

                    # [Check 3] í…ìŠ¤íŠ¸ ë³´ì¡° í™•ì¸
                    if not is_plus:
                        try:
                            container_text = offer_container.text
                            if "Plus" in container_text and ("í• ì¸" in container_text or "ì ˆì•½" in container_text):
                                is_plus = True
                        except: pass

                    # 6-3 ì›ê°€ íŒŒì‹±
                    original_price = current_price
                    try:
                        orig_elem = offer_container.find_element(By.CSS_SELECTOR, f"[data-qa='mfeCtaMain#offer{i}#originalPrice']")
                        raw_orig = orig_elem.get_attribute("textContent").strip()
                        original_price = int(re.sub(r'[^0-9]', '', raw_orig))
                    except: pass # ì›ê°€ê°€ ì—†ìœ¼ë©´ ì •ê°€ íŒë§¤

                    # í• ì¸ìœ¨ ê³„ì‚°
                    discount_rate = 0
                    if original_price > current_price:
                        discount_rate = int(((original_price - current_price) / original_price) * 100)

                    # 6-4 ì¢…ë£Œì¼ íŒŒì‹±
                    sale_end_date = None
                    try:
                        date_elem = offer_container.find_element(By.CSS_SELECTOR, f"[data-qa='mfeCtaMain#offer{i}#discountDescriptor']")
                        raw_date_text = date_elem.get_attribute("textContent")

                        # "2025/12/22" ë˜ëŠ” "2025.12.22" ë“±ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                        date_nums = re.findall(r'\d+', raw_date_text)
                        if len(date_nums) >= 3:
                            # ì—°ë„ê°€ 2ìë¦¬ì¸ ê²½ìš° ì²˜ë¦¬ (ë³´í†µ 4ìë¦¬)
                            year = date_nums[0] if len(date_nums[0]) == 4 else f"20{date_nums[0]}"
                            sale_end_date = f"{year}-{date_nums[1].zfill(2)}-{date_nums[2].zfill(2)}"
                    except: pass

                    # 6-5 ìµœì €ê°€ ë¹„êµ
                    if current_price < best_price:
                        best_price = current_price
                        best_offer_data = {
                            "originalPrice": original_price,
                            "currentPrice": current_price,
                            "discountRate": discount_rate,
                            "saleEndDate": sale_end_date,
                            "isPlusExclusive": is_plus
                        }
                        found_valid_offer = True

                except Exception:
                    continue

        # [ë°ì´í„° ì—†ìŒ ì²˜ë¦¬]
        if not found_valid_offer or best_offer_data is None:
            logger.warning(f"ğŸš« Skip: Valid price not found for {title}")
            return

        # 5. ì „ì†¡
        image_url = ""
        try:
            img_elem = driver.find_element(By.CSS_SELECTOR, "img[data-qa='gameBackgroundImage#heroImage#image']")
            image_url = img_elem.get_attribute("src").split("?")[0]
        except: pass

        ps_store_id = target_url.split("/")[-1].split("?")[0]

        payload = {
            "psStoreId": ps_store_id,
            "title": title,
            "publisher": "Batch Crawler",
            "imageUrl": image_url,
            "description": "Full Data Crawler",
            "originalPrice": best_offer_data["originalPrice"],
            "currentPrice": best_offer_data["currentPrice"],
            "discountRate": best_offer_data["discountRate"],
            "saleEndDate": best_offer_data["saleEndDate"],
            "isPlusExclusive": best_offer_data["isPlusExclusive"], # ì´ì œ ì •ìƒì ìœ¼ë¡œ True/Falseê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤
            "genreIds": genre_ids,
            "platforms": platforms
        }

        send_data_to_server(payload, title)

    except Exception as e:
        logger.error(f"   âš ï¸ Fatal Error processing {target_url}: {e}")

def send_data_to_server(payload, title):
    try:
        res = requests.post(JAVA_API_URL, json=payload, timeout=5)
        if res.status_code == 200:
            logger.info(f"   ğŸ“¤ Sent: {title} ({payload['currentPrice']} KRW)")
        else:
            logger.error(f"   ğŸ’¥ Server Error ({res.status_code}): {title}")
    except Exception as e:
        logger.error(f"   ğŸ’¥ Network Error sending {title}: {e}")

# --- [API ì—”ë“œí¬ì¸íŠ¸] ---
@app.route('/run', methods=['POST'])
def trigger_crawl():
    global is_running

    # Thread-safe Lock ì‚¬ìš©
    with lock:
        if is_running:
            return jsonify({"status": "error", "message": "Crawler is already running"}), 409
        is_running = True

    # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
    thread = threading.Thread(target=run_batch_crawler_logic)
    thread.daemon = True # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ê°™ì´ ì¢…ë£Œë˜ë„ë¡ ì„¤ì •
    thread.start()

    return jsonify({"status": "success", "message": "Crawler started in background"}), 200

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "UP", "crawler_running": is_running}), 200

if __name__ == "__main__":
    logger.info("ğŸ‘‚ [Collector] Server starting on port 5000...")
    app.run(host="0.0.0.0", port=5000)