import random
import os
import time
import json
import requests
import traceback
import re
import threading
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime
from flask import Flask, jsonify

import undetected_chromedriver as uc
from fake_useragent import UserAgent

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- [ì„¤ì • ë° ë¡œê¹… ì´ˆê¸°í™”] ---
if not os.path.exists('logs'):
    os.makedirs('logs')

log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
file_handler = RotatingFileHandler('logs/crawler.log', maxBytes=10*1024*1024, backupCount=5)
file_handler.setFormatter(log_formatter)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

logger = logging.getLogger("PS-Collector")
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

app = Flask(__name__)
session = requests.Session()
session.headers.update({'Connection': 'keep-alive'})

BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8080")
JAVA_API_URL = f"{BASE_URL}/api/v1/games/collect"
TARGET_API_URL = f"{BASE_URL}/api/v1/games/targets"
SELENIUM_URL = os.getenv("SELENIUM_URL")
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

lock = threading.Lock()
is_running = False

# =========================================================
# âš™ï¸ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì •
# =========================================================
# í™˜ê²½ë³€ìˆ˜ CRAWLER_MODEê°€ 'HIGH'ë©´ ê³ ì„±ëŠ¥, ì—†ìœ¼ë©´ 'LOW'(ì˜¤ë¼í´ í”„ë¦¬í‹°ì–´)
CURRENT_MODE = os.getenv("CRAWLER_MODE", "LOW").upper()

CONFIG = {
    "LOW": {  # ğŸ¢ ì˜¤ë¼í´ í”„ë¦¬í‹°ì–´
        "restart_interval": 50,         # 50ê°œë§ˆë‹¤ ì¬ì‹œì‘ (ë©”ëª¨ë¦¬ ë³´í˜¸)
        "page_load_strategy": "none",   # ë¡œë”© ëŒ€ê¸° ì•ˆ í•¨ (ì§ì ‘ ì œì–´)
        "sleep_min": 3.0,               # ìµœì†Œ 3ì´ˆ ëŒ€ê¸° (CPU ì•ˆì •í™”)
        "sleep_max": 5.0,               # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
        "timeout": 8,                   # íƒ€ì„ì•„ì›ƒ ì§§ê²Œ (ë¹ ë¥¸ ì†ì ˆ)
        "use_cdp_block": True,          # ì´ë¯¸ì§€/CSS ì°¨ë‹¨ (í•„ìˆ˜)
        "window_stop": True             # ê°•ì œ ë¡œë”© ì¤‘ë‹¨ ì‚¬ìš©
    },
    "HIGH": { # ğŸï¸ ê³ ì‚¬ì–‘ ì„œë²„/PC
        "restart_interval": 500,        # 500ê°œë§ˆë‹¤ ì¬ì‹œì‘
        "page_load_strategy": "normal", # ì •ìƒ ë¡œë”©
        "sleep_min": 1.5,               # 1.5ì´ˆ ëŒ€ê¸°
        "sleep_max": 2.5,               # 2.5ì´ˆ ëŒ€ê¸°
        "timeout": 15,                  # íƒ€ì„ì•„ì›ƒ ë„‰ë„‰íˆ
        "use_cdp_block": False,         # ì°¨ë‹¨ ì•ˆ í•¨
        "window_stop": False            # ê°•ì œ ì¤‘ë‹¨ ì•ˆ í•¨
    }
}

CONF = CONFIG.get(CURRENT_MODE, CONFIG["LOW"])

logger.info(f"ğŸ”§ Crawler Mode Initialized: [{CURRENT_MODE}]")
logger.info(f"   ğŸ‘‰ Strategy: {CONF['page_load_strategy']} | Restart: {CONF['restart_interval']}")
# =========================================================


def get_driver():
    """ë“œë¼ì´ë²„ ì„¤ì • (Hybrid Mode ì ìš©)"""
    ua = UserAgent()
    random_user_agent = ua.random
    logger.info(f"ğŸ­ Generated User-Agent: {random_user_agent}")

    w = random.randint(1800, 1920)
    h = random.randint(950, 1080)
    random_window_size = f"{w},{h}"

    driver = None

    # [ê³µí†µ] ê¸°ë³¸ ìµœì í™” ì˜µì…˜
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.popups": 2,
        "disk-cache-size": 4096
    }

    # [Case A] Docker / Selenium Grid
    if SELENIUM_URL:
        logger.info(f"ğŸŒ [Docker Mode] Grid: {SELENIUM_URL}")
        options = webdriver.ChromeOptions()

        # âš™ï¸ [Hybrid] ì„¤ì •ëœ ë¡œë”© ì „ëµ ì ìš©
        options.page_load_strategy = CONF['page_load_strategy']

        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={random_window_size}")

        # OCI ë¦¬ì†ŒìŠ¤ ì ˆì•½ í•„ìˆ˜ ì˜µì…˜
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-extensions")

        # LOW ëª¨ë“œì¼ ë•Œ ì¶”ê°€ ê²½ëŸ‰í™”
        if CURRENT_MODE == "LOW":
            options.add_argument("--disable-background-networking")
            options.add_argument("--disable-sync")

        options.add_experimental_option("prefs", prefs)
        driver = webdriver.Remote(command_executor=SELENIUM_URL, options=options)

    # [Case B] ë¡œì»¬ í™˜ê²½ (Undetected Chrome)
    else:
        logger.info(f"ğŸ’» [Local Mode] Starting Chrome ({CURRENT_MODE} Spec)")
        options = uc.ChromeOptions()

        # âš™ï¸ [Hybrid] ì„¤ì •ëœ ë¡œë”© ì „ëµ ì ìš©
        options.page_load_strategy = CONF['page_load_strategy']

        if os.getenv("HEADLESS", "false").lower() == "true":
             options.add_argument("--headless=new")

        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={random_window_size}")
        options.add_argument("--disable-popup-blocking")

        # OCI ë¡œì»¬ ì‹¤í–‰ ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")

        driver = uc.Chrome(options=options, use_subprocess=True)

    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    # âš™ï¸ [Hybrid] CDP ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ (ì„¤ì •ëœ ê²½ìš°ë§Œ)
    if CONF["use_cdp_block"]:
        try:
            driver.execute_cdp_cmd("Network.setBlockedURLs", {
                "urls": ["*.png", "*.jpg", "*.gif", "*.webp", "*.css", "*.woff", "*.woff2", "*google-analytics*"]
            })
            driver.execute_cdp_cmd("Network.enable", {})
            logger.info("   ğŸ›¡ï¸ Network filtering enabled (Images/Fonts/CSS blocked)")
        except Exception as e:
            logger.warning(f"   âš ï¸ CDP Optimization skipped: {e}")

    return driver

def fetch_update_targets():
    try:
        res = session.get(TARGET_API_URL, timeout=30)
        if res.status_code == 200:
            targets = res.json()
            logger.info(f"ğŸ“¥ Received {len(targets)} targets.")
            return targets
        return []
    except Exception as e:
        logger.error(f"âŒ Connection Error: {e}")
        return []

def mine_english_title(driver):
    try:
        src = driver.page_source
        match = re.search(r'"invariantName"\s*:\s*"([^"]+)"', src)
        if match:
            raw_title = match.group(1)
            try: raw_title = raw_title.encode('utf-8').decode('unicode_escape')
            except: pass
            try: raw_title = raw_title.encode('latin1').decode('utf-8')
            except: pass

            raw_title = raw_title.replace("â€™", "'").replace("â€˜", "'")
            raw_title = re.sub(r'[â„¢Â®Ã¢Â¢]', '', raw_title)
            logger.info(f"   ğŸ’ Mined Invariant Title: {raw_title}")
            return raw_title.strip()
        return None
    except: return None

def send_discord_summary(total_scanned, deals_list):
    if not DISCORD_WEBHOOK_URL: return
    try:
        total_deals = len(deals_list)
        if total_deals == 0: return

        sorted_deals = sorted(deals_list, key=lambda x: x['discountRate'], reverse=True)
        top_5 = sorted_deals[:5]

        message = f"## ğŸ“¢ [PS-Tracker] ì¼ì¼ ìˆ˜ì§‘ ë¦¬í¬íŠ¸ ({CURRENT_MODE})\n"
        message += f"**ğŸ—“ï¸ ë‚ ì§œ:** {datetime.now().strftime('%Y-%m-%d')}\n"
        message += f"**ğŸ“Š í†µê³„:** ì´ `{total_scanned}`ê°œ ìŠ¤ìº” / **`{total_deals}`**ê°œ í• ì¸ ê°ì§€!\n"
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"

        for i, game in enumerate(top_5, 1):
            sale_price = "{:,}".format(game['currentPrice'])
            message += f"{i}ï¸âƒ£ **[{game['discountRate']}%] {game['title']}**\n"
            message += f"ã€€ ğŸ’° **â‚©{sale_price}**\n"
            if i < len(top_5): message += "â”€â”€â”€\n"

        message += "\n[ğŸ”— ì‹¤ì‹œê°„ ìµœì €ê°€ í™•ì¸í•˜ê¸°](https://ps-signal.com)"
        requests.post(DISCORD_WEBHOOK_URL, json={"content": message})
    except: pass

def run_batch_crawler_logic():
    global is_running
    logger.info(f"ğŸš€ [Crawler] Batch job started - Mode: {CURRENT_MODE}")

    driver = None
    total_processed_count = 0
    collected_deals = []

    try:
        driver = get_driver()
        wait = WebDriverWait(driver, CONF['timeout']) # ì„¤ì •ëœ íƒ€ì„ì•„ì›ƒ ì‚¬ìš©
        visited_urls = set()

        # [Phase 1] ê¸°ì¡´ íƒ€ê²Ÿ ê°±ì‹ 
        targets = fetch_update_targets()
        if targets:
            logger.info(f"ğŸ”„ [Phase 1] Updating {len(targets)} tracked games...")

            for i, url in enumerate(targets):
                if not is_running: break

                # âš™ï¸ [Hybrid] ì„¤ì •ëœ ì£¼ê¸°ë¡œ ì¬ì‹œì‘
                if i > 0 and i % CONF["restart_interval"] == 0:
                    logger.info(f"â™»ï¸ [Phase 1] Memory Cleanup at item {i}...")
                    try: driver.quit()
                    except: pass
                    time.sleep(5)
                    driver = get_driver()
                    wait = WebDriverWait(driver, CONF['timeout'])

                deal_info = crawl_detail_and_send(driver, wait, url)

                if deal_info:
                    total_processed_count += 1
                    if deal_info.get('discountRate', 0) > 0:
                        collected_deals.append(deal_info)
                visited_urls.add(url)

                # âš™ï¸ [Hybrid] ì„¤ì •ëœ íœ´ì‹ ì‹œê°„
                if random.random() < 0.3: # 30% í™•ë¥ ë¡œ íœ´ì‹
                    time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

        # [Phase 2] ì‹ ê·œ íƒìƒ‰
        if is_running:
            logger.info(f"ğŸ”­ [Phase 2] Starting Deep Discovery...")
            base_category_path = "https://store.playstation.com/ko-kr/category/3f772501-f6f8-49b7-abac-874a88ca4897"
            search_params = "?FULL_GAME=storeDisplayClassification&GAME_BUNDLE=storeDisplayClassification&PREMIUM_EDITION=storeDisplayClassification"

            current_page = 1
            max_pages = 15

            while current_page <= max_pages:
                if not is_running: break

                # LOW ëª¨ë“œ(ì˜¤ë¼í´): 2í˜ì´ì§€ë§ˆë‹¤ ì¬ì‹œì‘ (2í˜ì´ì§€ x 24ê°œ = ì•½ 48ê°œ ê²Œì„ â†’ Phase 1ì˜ 50ê°œ ì œí•œê³¼ ë¹„ìŠ·)
                # HIGH ëª¨ë“œ: 20í˜ì´ì§€ë§ˆë‹¤ ì¬ì‹œì‘
                p2_restart = 2 if CURRENT_MODE == "LOW" else 20

                if current_page > 1 and current_page % p2_restart == 0:
                     logger.info("â™»ï¸ [Maintenance] Restarting driver...")
                     try: driver.quit()
                     except: pass
                     time.sleep(5)
                     driver = get_driver()
                     wait = WebDriverWait(driver, CONF['timeout'])

                target_list_url = f"{base_category_path}/{current_page}{search_params}"
                logger.info(f"   ğŸ“– Scanning Page {current_page}/{max_pages}")

                try:
                    driver.get(target_list_url)

                    # âš™ï¸ [Hybrid] ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì ‘ê·¼ ìµœì í™”
                    time.sleep(2.0) # JSON ë¡œë”© ëŒ€ê¸°
                    if CONF["window_stop"]:
                        driver.execute_script("window.stop();")

                    # ìš”ì†Œ í™•ì¸
                    try:
                        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='/product/']")))
                    except TimeoutException:
                        driver.refresh()
                        time.sleep(3)

                    # ìŠ¤í¬ë¡¤
                    driver.execute_script(f"window.scrollTo(0, {random.randint(800, 1200)});")
                    time.sleep(1)

                except Exception as e:
                    logger.warning(f"âš ï¸ Page Load Error on {current_page}: {e}")

                # ë§í¬ ìˆ˜ì§‘
                page_candidates = []
                try:
                    link_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/product/']")
                    for el in link_elements:
                        url = el.get_attribute("href")
                        if url and "/ko-kr/product/" in url and url not in visited_urls:
                            if url not in page_candidates: page_candidates.append(url)
                except: pass

                if not page_candidates: break

                for url in page_candidates:
                    if not is_running: break
                    deal_info = crawl_detail_and_send(driver, wait, url)
                    if deal_info:
                        total_processed_count += 1
                        if deal_info.get('discountRate', 0) > 0:
                            collected_deals.append(deal_info)
                    visited_urls.add(url)
                    # Phase 2ëŠ” ì¡°ê¸ˆ ë” ë¹¨ë¦¬ ë„˜ì–´ê°€ë„ ë¨
                    time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

                current_page += 1

            send_discord_summary(total_processed_count, collected_deals)

    except Exception as e:
        logger.error(f"ğŸ”¥ Critical Crawler Error: {e}")
        logger.error(traceback.format_exc())
    finally:
        if driver:
            try: driver.quit()
            except: pass
        with lock: is_running = False

def crawl_detail_and_send(driver, wait, target_url):
    try:
        # âš™ï¸ [Hybrid] ì ‘ê·¼ ë°©ì‹ ìµœì í™”
        driver.get(target_url)

        # 1. ì•ˆì „ ë§ˆì§„ ëŒ€ê¸°
        time.sleep(CONF["sleep_min"])

        # 2. ê°•ì œ ë¡œë”© ì¤‘ë‹¨ (LOW ëª¨ë“œì¼ ë•Œë§Œ ì‘ë™)
        if CONF["window_stop"]:
            try: driver.execute_script("window.stop();")
            except: pass

        # 3. ì œëª© ë¡œë”© (ì‹¤íŒ¨ì‹œ ë¹ ë¥¸ í¬ê¸°)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "[data-qa='mfe-game-title#name']")))
        except TimeoutException:
            # 1ì°¨ ì‹¤íŒ¨ ì‹œ: ë¡œê·¸ ì°ê³  ìƒˆë¡œê³ ì¹¨ ì‹œë„
            logger.warning(f"   âš ï¸ Timeout (1st). Retrying refresh... : {target_url}")

            try:
                driver.refresh() # ì‹¬íì†Œìƒìˆ !
                time.sleep(3.0)  # ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ 3ì´ˆ ëŒ€ê¸°

                # ê°•ì œ ì¤‘ë‹¨ (2ì°¨ - ìƒˆë¡œê³ ì¹¨ í–ˆìœ¼ë‹ˆ ë‹¤ì‹œ ëŠì–´ì¤˜ì•¼ í•¨)
                if CONF["window_stop"]:
                    try: driver.execute_script("window.stop();")
                    except: pass

                # 2ì°¨ ì‹œë„: ë‹¤ì‹œ ì œëª© ì°¾ê¸°
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "[data-qa='mfe-game-title#name']")))
                logger.info(f"   âœ… Recovered after refresh!")

            except TimeoutException:
                # 2ë²ˆ í•´ë„ ì•ˆ ë˜ë©´ ì§„ì§œ ì•ˆ ë˜ëŠ” ê±°ì„ -> ì¿¨í•˜ê²Œ í¬ê¸°
                logger.error(f"   âŒ Final Timeout (Give up): {target_url}")
                return None

        # 4. ê°€ê²© ì»¨í…Œì´ë„ˆ (ì—†ìœ¼ë©´ íŒ¨ìŠ¤)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "[data-qa^='mfeCtaMain#offer']")))
        except:
            logger.info("   â„¹ï¸ No price container found")
            pass

        english_title = mine_english_title(driver)

        try:
            title = driver.find_element(By.CSS_SELECTOR, "[data-qa='mfe-game-title#name']").text.strip()
        except:
            title = "Unknown Title"

        genre_ids = ""
        try:
            genre_element = driver.find_element(By.CSS_SELECTOR, "[data-qa='gameInfo#releaseInformation#genre-value']")
            genre_ids = genre_element.text
        except: pass

        platform_set = set()
        try:
            tag_elements = driver.find_elements(By.CSS_SELECTOR, "[data-qa^='mfe-game-title#productTag']")
            for el in tag_elements:
                raw_text = el.get_attribute("textContent").strip().upper()
                if "PS5" in raw_text: platform_set.add("PS5")
                if "PS4" in raw_text: platform_set.add("PS4")
        except: pass
        platforms = list(platform_set)

        best_price = float('inf')
        best_offer_data = None
        found_valid_offer = False

        # DOM íŒŒì‹±ì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
        time.sleep(0.5)

        for i in range(3):
            try:
                offer_selector = f"[data-qa='mfeCtaMain#offer{i}']"
                try: offer_container = driver.find_element(By.CSS_SELECTOR, offer_selector)
                except: continue

                try:
                    price_selector = f"[data-qa='mfeCtaMain#offer{i}#finalPrice']"
                    price_elem = offer_container.find_element(By.CSS_SELECTOR, price_selector)
                    raw_price = price_elem.get_attribute("textContent").strip()
                    clean_price_text = re.sub(r'[^0-9]', '', raw_price)
                    if not clean_price_text: continue
                    current_price = int(clean_price_text)
                    if current_price == 0: continue
                except: continue

                is_plus = False
                try:
                    if offer_container.find_elements(By.CSS_SELECTOR, ".psw-c-t-ps-plus"): is_plus = True
                except: pass

                original_price = current_price
                found_original = False
                try:
                    orig_elem = offer_container.find_element(By.CSS_SELECTOR, f"[data-qa='mfeCtaMain#offer{i}#originalPrice']")
                    parsed_orig = int(re.sub(r'[^0-9]', '', orig_elem.get_attribute("textContent").strip()))
                    if parsed_orig > current_price:
                        original_price = parsed_orig
                        found_original = True
                except: pass

                if not found_original:
                    try:
                        strikethrough_elems = offer_container.find_elements(By.CSS_SELECTOR, ".psw-t-strike")
                        for elem in strikethrough_elems:
                            parsed_price = int(re.sub(r'[^0-9]', '', elem.get_attribute("textContent").strip()))
                            if parsed_price > current_price:
                                original_price = parsed_price
                                break
                    except: pass

                discount_rate = 0
                if original_price > current_price:
                    discount_rate = int(round(((original_price - current_price) / original_price) * 100))

                sale_end_date = None
                try:
                    date_elem = offer_container.find_element(By.CSS_SELECTOR, f"[data-qa='mfeCtaMain#offer{i}#discountDescriptor']")
                    match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', date_elem.get_attribute("textContent"))
                    if match: sale_end_date = f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
                except: pass

                if current_price < best_price:
                    best_price = current_price
                    best_offer_data = {
                        "originalPrice": original_price,
                        "currentPrice": current_price,
                        "discountRate": discount_rate,
                        "saleEndDate": sale_end_date,
                        "isPlusExclusive": is_plus
                    }
                    found_valid_offer = True
            except: continue

        if not found_valid_offer or best_offer_data is None:
            logger.warning(f"ğŸš« Skip: Valid price not found for {title}")
            return

        image_url = ""
        try:
            scripts = driver.find_elements(By.CSS_SELECTOR, "script[type='application/json']")
            for script in scripts:
                content = script.get_attribute("innerHTML")
                if "media" not in content: continue
                try:
                    data = json.loads(content)
                    cache = data.get("cache", {})
                    for val in cache.values():
                        if "personalizedMeta" in val and "media" in val["personalizedMeta"]:
                            for media in val["personalizedMeta"]["media"]:
                                if media.get("role") == "MASTER":
                                    image_url = media.get("url"); break
                                if media.get("role") == "GAMEHUB_COVER_ART" and not image_url:
                                    image_url = media.get("url")
                            if image_url: break
                    if image_url: break
                except: pass
            if image_url: logger.info(f"   ğŸ“¸ Image found via JSON Script")
        except: pass

        if not image_url:
            try:
                image_url = driver.find_element(By.CSS_SELECTOR, "meta[property='og:image']").get_attribute("content").split("?")[0]
            except: pass

        ps_store_id = target_url.split("/")[-1].split("?")[0]
        payload = {
            "psStoreId": ps_store_id,
            "title": title,
            "englishTitle": english_title,
            "publisher": "Batch Crawler",
            "imageUrl": image_url,
            "description": "Full Data Crawler",
            "originalPrice": best_offer_data["originalPrice"],
            "currentPrice": best_offer_data["currentPrice"],
            "discountRate": best_offer_data["discountRate"],
            "saleEndDate": best_offer_data["saleEndDate"],
            "isPlusExclusive": best_offer_data["isPlusExclusive"],
            "genreIds": genre_ids,
            "platforms": platforms
        }

        send_data_to_server(payload, title)
        return payload

    except Exception as e:
        logger.error(f"   âš ï¸ Fatal Error processing {target_url}: {e}")
        return None

def send_data_to_server(payload, title):
    try:
        res = session.post(JAVA_API_URL, json=payload, timeout=30)
        if res.status_code == 200:
            logger.info(f"   ğŸ“¤ Sent: {title} ({payload['currentPrice']} KRW)")
        else:
            logger.error(f"   ğŸ’¥ Server Error ({res.status_code}): {title}")
    except:
        logger.error(f"   ğŸ’¥ Network Error sending {title}")

@app.route('/run', methods=['POST'])
def trigger_crawl():
    global is_running
    with lock:
        if is_running: return jsonify({"status": "error", "message": "Crawler is already running"}), 409
        is_running = True
    thread = threading.Thread(target=run_batch_crawler_logic)
    thread.daemon = True
    thread.start()
    return jsonify({"status": "success", "message": "Crawler started"}), 200

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "UP", "crawler_running": is_running, "mode": CURRENT_MODE}), 200

if __name__ == "__main__":
    logger.info(f"ğŸ‘‚ [Collector] Server starting on port 5000 (Mode: {CURRENT_MODE})")
    app.run(host="0.0.0.0", port=5000)