import random
import os
import time
import re
import threading
import logging
import traceback
import gc
from logging.handlers import RotatingFileHandler
from datetime import datetime

# [Playwright Imports]
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

from flask import Flask, jsonify, request
import requests

# --- [1. ì„¤ì • ë° ë¡œê¹… ì´ˆê¸°í™”] ---
if not os.path.exists('logs'):
    os.makedirs('logs')

log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
file_handler = RotatingFileHandler('logs/crawler.log', maxBytes=10*1024*1024, backupCount=5)
file_handler.setFormatter(log_formatter)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

logger = logging.getLogger("PS-Collector")
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

logging.getLogger('werkzeug').setLevel(logging.ERROR)

app = Flask(__name__)
session = requests.Session()
session.headers.update({'Connection': 'keep-alive'})

# [í™˜ê²½ ë³€ìˆ˜]
BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8080")
JAVA_API_URL = f"{BASE_URL}/api/v1/games/collect"
TARGET_API_URL = f"{BASE_URL}/api/v1/games/targets"
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

lock = threading.Lock()
is_running = False

# [ì„¤ì • ìœ ì§€]
CURRENT_MODE = os.getenv("CRAWLER_MODE", "LOW").upper()
CONFIG = {
    "LOW": {
        "restart_interval": 10,
        "timeout": 60000,
        "sleep_min": 3.0,
        "sleep_max": 6.0,
        "block_fonts": True,
    },
    "HIGH": {
        "restart_interval": 200,
        "timeout": 30000,
        "sleep_min": 1.0,
        "sleep_max": 3.0,
        "block_fonts": False,
    }
}
CONF = CONFIG.get(CURRENT_MODE, CONFIG["LOW"])
logger.info(f"ğŸ”§ Crawler Config: {CURRENT_MODE} | Engine: Playwright (Manual Stealth)")


# --- [2. ë¸Œë¼ìš°ì € ë° í˜ì´ì§€ ì„¤ì •] ---
def create_browser_context(p):
    """ë¸Œë¼ìš°ì € ì‹¤í–‰ ë° ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (ìµœì í™” ë²„ì „)"""

    # User-Agent ë¦¬ìŠ¤íŠ¸
    DESKTOP_USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36 Edg/144.0.0.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36"
    ]
    user_agent = random.choice(DESKTOP_USER_AGENTS)

    browser = p.chromium.launch(
        headless=True,
        args=[
            "--no-sandbox",
            "--disable-setuid-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu",
            "--disable-extensions",
            "--disable-blink-features=AutomationControlled"
        ]
    )

    context = browser.new_context(
        user_agent=user_agent,
        viewport={"width": 1920, "height": 1080},
        locale="ko-KR",
        timezone_id="Asia/Seoul"
    )

    return browser, context

def setup_page(context):
    """í˜ì´ì§€ ìƒì„± ë° ìˆ˜ë™ ìŠ¤í…”ìŠ¤ ì ìš©"""
    page = context.new_page()

    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ì‹  ìˆ˜ë™ ìŠ¤í…”ìŠ¤ ì ìš©
    page.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined
        });
    """)

    # [ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨] ì„¤ì •ì— ë”°ë¼ í°íŠ¸ ì°¨ë‹¨ ì—¬ë¶€ ê²°ì •
    def route_intercept(route):
        r_type = route.request.resource_type

        # ì´ë¯¸ì§€, ë¯¸ë””ì–´ëŠ” ë¬´ì¡°ê±´ ì°¨ë‹¨ (ì†ë„)
        if r_type in ["image", "media"]:
            route.abort()
            return

        # í°íŠ¸: ì„¤ì •ì— ë”°ë¼ ì°¨ë‹¨ (LOW ëª¨ë“œ)
        if CONF.get("block_fonts", False) and r_type == "font":
            route.abort()
            return

        route.continue_()

    page.route("**/*", route_intercept)
    return page

def mine_english_title(html_content):
    try:
        match = re.search(r'"invariantName"\s*:\s*"([^"]+)"', html_content)
        if match:
            raw_title = match.group(1)
            try: raw_title = raw_title.encode('utf-8').decode('unicode_escape')
            except: pass
            raw_title = raw_title.replace("â€™", "'").replace("â€˜", "'")
            return re.sub(r'[â„¢Â®Ã¢Â¢]', '', raw_title).strip()
    except: return None
    return None

def crawl_detail_and_send(page, target_url, verbose=False):
    try:
        page.goto(target_url, timeout=CONF['timeout'], wait_until="commit")

        # ë¦¬ë‹¤ì´ë ‰íŠ¸ URLë¡œ ë‹¨ì¢… ì˜ì‹¬ ê²Œì„ ê°ì§€
        if "/error" in page.url:
            logger.warning(f"ğŸš¨ ë‹¨ì¢… ì˜ì‹¬ (URL ë¦¬ë‹¤ì´ë ‰íŠ¸): {target_url}")
            return {"is_delisted": True, "ps_store_id": target_url.split("/")[-1].split("?")[0]}

        # 1. ì œëª© ë¡œë”© ëŒ€ê¸°
        try:
            page.wait_for_selector("[data-qa='mfe-game-title#name']", state="attached", timeout=30000)
        except PlaywrightTimeoutError:
            logger.warning(f"â³ Title Load Timeout: {target_url}")
            try:
                page.reload(wait_until="commit")
                # 2ì°¨ ì‹œë„: 20ì´ˆ ì¶”ê°€ ëŒ€ê¸°
                page.wait_for_selector("[data-qa='mfe-game-title#name']", state="attached", timeout=20000)
                logger.info("   â™»ï¸ Reloaded & Found title!")
            except PlaywrightTimeoutError:
                # 2ë²ˆ ë‹¤ ì‹¤íŒ¨í•˜ë©´ ì§„ì§œ ì‹¤íŒ¨
                logger.error(f"âŒ Final Title Timeout: {target_url}")
                return None

        # 2. ë°ì´í„° ì¶”ì¶œ
        try:
            title = page.locator("[data-qa='mfe-game-title#name']").inner_text().strip()
        except Exception as e:
            logger.error(f"âŒ Found selector but failed to extract title text: {e}")
            return None

        english_title = mine_english_title(page.content())

        publisher = "Batch Crawler"
        try:
            publisher_loc = page.locator("[data-qa='mfe-game-title#publisher']")

            # ìš”ì†Œê°€ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if publisher_loc.count() > 0:
                publisher = publisher_loc.first.inner_text().strip()

        except Exception as e:
            logger.warning(f"   âš ï¸ Publisher extraction failed: {e}")

        # 3. ê°€ê²© ì •ë³´ë‚˜ êµ¬ë§¤ ë²„íŠ¼ ì˜ì—­ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
        try:
            # ê°€ê²© ì •ë³´ë‚˜ êµ¬ë§¤ ë²„íŠ¼ ì˜ì—­ì´ ëœ° ë•Œê¹Œì§€ ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
            page.wait_for_selector("[data-qa^='mfeCtaMain#offer']", timeout=15000)
        except:
            # 5ì´ˆ ê¸°ë‹¤ë ¤ë„ ì•ˆ ëœ¨ë©´, ì§„ì§œ ì—†ëŠ” ê±°ê±°ë‚˜ ë¬´ë£Œ ê²Œì„ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê·¸ëƒ¥ ì§„í–‰
            pass

        # í”Œë«í¼
        platform_set = set()
        try:
            tag_elements = page.locator("[data-qa^='mfe-game-title#productTag']").all()
            for el in tag_elements:
                raw_text = el.text_content().strip().upper()
                if "PS5" in raw_text: platform_set.add("PS5")
                if "PS4" in raw_text: platform_set.add("PS4")
                if "VR2" in raw_text: platform_set.add("PS_VR2")
                elif "VR" in raw_text: platform_set.add("PS_VR")
            platforms = list(platform_set)
        except: platforms = []

        genre_ids = ""
        try:
            genre_ids = page.locator("[data-qa='gameInfo#releaseInformation#genre-value']").inner_text()
        except: pass

        # ì¶œì‹œì¼ ë¡œì§
        release_date = None
        try:
            release_loc = page.locator("[data-qa='gameInfo#releaseInformation#releaseDate-value']")
            if release_loc.count() > 0:
                raw_date = release_loc.first.inner_text().strip()

                # '2025/10/9' ê°™ì€ í¬ë§·ì„ '2025-10-09' (yyyy-MM-dd)ë¡œ ë³€í™˜
                parts = raw_date.split("/")
                if len(parts) == 3:
                    year = parts[0]
                    month = parts[1].zfill(2) # '10' -> '10', '5' -> '05'
                    day = parts[2].zfill(2)   # '9' -> '09', '20' -> '20'
                    release_date = f"{year}-{month}-{day}"
                else:
                    # í˜¹ì‹œ / í˜•íƒœê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì˜ˆì™¸ì ì¸ ë‚ ì§œê°€ ë“¤ì–´ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆì „ë§
                    release_date = raw_date.replace("/", "-")
        except Exception as e:
            logger.warning(f"   âš ï¸ Release Date extraction failed: {e}")

        # 4. ê°€ê²© ë¡œì§
        best_offer_data = None
        min_price = float('inf')
        is_in_catalog_global = False

        time.sleep(0.5)

        for i in range(3):
            try:
                offer_loc = page.locator(f"[data-qa='mfeCtaMain#offer{i}']")
                if not offer_loc.is_visible(): continue

                offer_text = offer_loc.inner_text()

                try:
                    radio = offer_loc.locator("input[type='radio']")
                    if radio.count() > 0:
                        val = radio.get_attribute("value")
                        if val and "UPSELL_PS_PLUS_GAME_CATALOG" in val:
                            is_in_catalog_global = True
                except: pass

                if not is_in_catalog_global:
                    if "ê²Œì„ ì¹´íƒˆë¡œê·¸" in offer_text or "ìŠ¤í˜ì…œì— ê°€ì…" in offer_text:
                        is_in_catalog_global = True

                try:
                    price_loc = offer_loc.locator(f"[data-qa='mfeCtaMain#offer{i}#finalPrice']")
                    if not price_loc.is_visible(): continue
                    price_text = price_loc.inner_text().strip()
                    current_price = int(re.sub(r'[^0-9]', '', price_text))
                    if current_price == 0: continue
                except: continue

                original_price = current_price
                try:
                    orig_loc = offer_loc.locator(f"[data-qa='mfeCtaMain#offer{i}#originalPrice']")
                    if orig_loc.is_visible():
                        orig_text = orig_loc.inner_text()
                        original_price = int(re.sub(r'[^0-9]', '', orig_text))
                except: pass

                is_plus_exclusive = False
                try:
                    if offer_loc.locator(".psw-c-t-ps-plus").count() > 0:
                        is_plus_exclusive = True
                except: pass

                sale_end_date = None
                try:
                    desc_loc = offer_loc.locator(f"[data-qa='mfeCtaMain#offer{i}#discountDescriptor']")
                    if desc_loc.is_visible():
                        desc_text = desc_loc.inner_text()
                        match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', desc_text)
                        if match:
                            sale_end_date = f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
                except: pass

                if current_price < min_price:
                    min_price = current_price
                    discount_rate = 0
                    if original_price > current_price:
                        discount_rate = int(round(((original_price - current_price) / original_price) * 100))

                    best_offer_data = {
                        "originalPrice": original_price,
                        "currentPrice": current_price,
                        "discountRate": discount_rate,
                        "saleEndDate": sale_end_date,
                        "isPlusExclusive": is_plus_exclusive
                    }
            except: continue

        if not best_offer_data:
            if is_in_catalog_global:
                logger.info(f"   â„¹ï¸ Catalog Only: {title}")
            else:
                logger.warning(f"   âš ï¸ No price offer found for: {title} (Page loaded but parsing failed)")
            return None

        # 5. ì´ë¯¸ì§€ URL (ì°¨ë‹¨í–ˆì§€ë§Œ ì†ì„±ì€ ì¡´ì¬í•  ìˆ˜ ìˆìŒ)
        image_url = ""
        try:
            #  ì •ê·œì‹ìœ¼ë¡œ HTML ì†ŒìŠ¤ ì „ì²´ì—ì„œ URL íŒ¨í„´ ì°¾ê¸°
            html_content = page.content()
            match = re.search(r'(https://image\.api\.playstation\.com/vulcan/[^"\'\s>]+)', html_content)

            if match:
                image_url = match.group(1).split("?")[0]

            # ì •ê·œì‹ ì‹¤íŒ¨ ì‹œ, DOM ë°©ì‹ ì‹œë„
            if not image_url:
                img_loc = page.locator("img[data-qa='gameBackgroundImage#heroImage#image']")
                if img_loc.count() > 0:
                    src = img_loc.first.get_attribute("src")
                    if src: image_url = src.split("?")[0]

        except Exception as e:
            logger.warning(f"   âš ï¸ Image Extraction Failed: {e}")

        ps_store_id = target_url.split("/")[-1].split("?")[0]

        payload = {
            "psStoreId": ps_store_id,
            "title": title,
            "englishTitle": english_title,
            "publisher": publisher,
            "imageUrl": image_url,
            "description": "Full Data Crawler",
            "genreIds": genre_ids,
            "releaseDate": release_date,
            "originalPrice": best_offer_data["originalPrice"],
            "currentPrice": best_offer_data["currentPrice"],
            "discountRate": best_offer_data["discountRate"],
            "saleEndDate": best_offer_data["saleEndDate"],
            "isPlusExclusive": best_offer_data["isPlusExclusive"],
            "inCatalog": is_in_catalog_global,
            "platforms": platforms
        }

        if verbose:
            logger.info(f"   ğŸ§ [Parsed Data Check] {title}")
            logger.info(f"      ğŸ“¸ ImageURL : {payload['imageUrl']}" if payload['imageUrl'] else "      ğŸ“¸ ImageURL : None")
            logger.info(f"      ğŸ·ï¸ Genres   : {payload['genreIds']}")
            logger.info(f"      ğŸ¢ Publisher: {payload['publisher']}")
            logger.info(f"      ğŸ“… Release  : {payload['releaseDate']}")
            logger.info(f"      ğŸ’° Discount : {payload['discountRate']}% (PlusOnly: {payload['isPlusExclusive']})")
            logger.info(f"      ğŸ“š Catalog  : {payload['inCatalog']}")
            logger.info(f"      --------------------------------------------------")

        send_data_to_server(payload, title)
        return payload

    except Exception as e:
        logger.error(f"   ğŸ”¥ Error: {target_url} -> {e}")
        return None

def send_data_to_server(payload, title):
    try:
        res = session.post(JAVA_API_URL, json=payload, timeout=30)
        if res.status_code == 200:
            logger.info(f"   ğŸ“¤ Sent: {title} ({payload['currentPrice']} KRW)")
        else:
            logger.error(f"   ğŸ’¥ Server Error ({res.status_code}): {title}")
    except requests.exceptions.Timeout:
        logger.error(f"   â³ Timeout Error: Server took too long to respond for {title}")
    except Exception as e:
        logger.error(f"   ğŸ’¥ Network Error sending {title}: {e}")

def fetch_update_targets():
    try:
        res = session.get(TARGET_API_URL, timeout=10)
        if res.status_code == 200:
            targets = res.json()
            logger.info(f"ğŸ“¥ Received {len(targets)} targets.")
            return targets
    except Exception as e:
        logger.error(f"âŒ Connection Error: {e}")
    return []

def send_discord_summary(total_scanned, deals_list, delisted_games):
    if not DISCORD_WEBHOOK_URL: return
    try:
        total_deals = len(deals_list)
        total_delisted = len(delisted_games)

        if total_deals == 0 and total_delisted == 0:
            logger.info("ğŸ“­ No deals or delisted games found today. Skipping Discord report.")
            return

        message = f"## ğŸ“¢ [PS-Tracker] ì¼ì¼ ìˆ˜ì§‘ ë¦¬í¬íŠ¸ ({CURRENT_MODE})\n"
        message += f"**ğŸ—“ï¸ ë‚ ì§œ:** {datetime.now().strftime('%Y-%m-%d')}\n"
        message += f"**ğŸ“Š í†µê³„:** ì´ `{total_scanned}`ê°œ ìŠ¤ìº” / **`{total_deals}`**ê°œ í• ì¸ ê°ì§€! ğŸ”¥\n"
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

        if total_delisted > 0:
            message += "ğŸš¨ **[ì£¼ì˜] ë‹¨ì¢… ì˜ì‹¬ ê²Œì„ (ìˆ˜ë™ ì‚­ì œ í•„ìš”)** ğŸš¨\n"
            for g in delisted_games:
                # ps_store_idë¥¼ ì¶œë ¥í•˜ì—¬ ê´€ë¦¬ì í˜ì´ì§€ì—ì„œ ì‰½ê²Œ ê²€ìƒ‰/ì‚­ì œí•  ìˆ˜ ìˆê²Œ ì œê³µ
                message += f"â€¢ ID: `{g['ps_store_id']}`\n"
            message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"

        if total_deals > 0:
            message += "**ğŸ† ì˜¤ëŠ˜ì˜ Top 5 í• ì¸**\n"
            sorted_deals = sorted(deals_list, key=lambda x: x['discountRate'], reverse=True)
            top_5 = sorted_deals[:5]

            for i, game in enumerate(top_5, 1):
                sale_price = "{:,}".format(game['currentPrice'])
                plat_list = game.get('platforms', [])
                plat_str = f" | `{'/'.join(plat_list)}`" if plat_list else ""
                message += f"{i}ï¸âƒ£ **[{game['discountRate']}%] {game['title']}**\n"
                message += f"ã€€ ğŸ’° **â‚©{sale_price}**{plat_str}\n"
                message += f"ã€€ â³ ~{game['saleEndDate'] or 'ìƒì‹œ ì¢…ë£Œ'}\n"
                if i < len(top_5): message += "â”€â”€â”€\n"

            message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            if total_deals > 5:
                message += f"ì™¸ **{total_deals - 5}**ê°œì˜ í• ì¸ì´ ë” ìˆìŠµë‹ˆë‹¤!\n"

        else:
            message += "ğŸ“­ ì˜¤ëŠ˜ì€ ìƒˆë¡œìš´ í• ì¸ì´ ì—†ìŠµë‹ˆë‹¤.\n"
            message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"

        message += "\n[ğŸ”— ì‹¤ì‹œê°„ ìµœì €ê°€ í™•ì¸í•˜ê¸°](https://ps-signal.com)"

        requests.post(DISCORD_WEBHOOK_URL, json={"content": message})
        logger.info("ğŸ”” Discord Summary Report sent!")

    except Exception as e:
        logger.error(f"âŒ Failed to send Discord summary: {e}")


# --- [4. ë©”ì¸ ì‹¤í–‰ ë¡œì§] ---
def run_batch_crawler_logic():
    global is_running
    logger.info(f"ğŸš€ [Crawler] Started. Mode: {CURRENT_MODE} (Safe Process Reset)")

    total_processed_count = 0
    collected_deals = []
    delisted_games = []

    try:
        visited_urls = set()

        # 1. íƒ€ê²Ÿ ê°€ì ¸ì˜¤ê¸°
        targets = fetch_update_targets()
        if not targets: targets = []

        # ------------------------------------------------------------------
        # [Phase 1] ê¸°ì¡´ íƒ€ê²Ÿ ê°±ì‹ 
        # ------------------------------------------------------------------
        if targets:
            logger.info(f"ğŸ”„ [Phase 1] Updating {len(targets)} tracked games...")

            BATCH_SIZE = CONF["restart_interval"]
            target_chunks = [targets[i:i + BATCH_SIZE] for i in range(0, len(targets), BATCH_SIZE)]

            for chunk_idx, chunk in enumerate(target_chunks):
                if not is_running: break

                logger.info(f"â™»ï¸ [Phase 1] Starting Batch {chunk_idx + 1}/{len(target_chunks)}")

                # ë°°ì¹˜ë§ˆë‹¤ ì—”ì§„(p)ì„ ìƒˆë¡œ ë§Œë“¤ê³  ë”
                try:
                    with sync_playwright() as p:
                        browser = None
                        context = None
                        try:
                            browser, context = create_browser_context(p)
                            page = setup_page(context)

                            for url in chunk:
                                if not is_running: break

                                res = crawl_detail_and_send(page, url)
                                if res:
                                    if res.get("is_delisted"):
                                        delisted_games.append(res)
                                    else:
                                        total_processed_count += 1
                                        if res.get('discountRate', 0) > 0: collected_deals.append(res)
                                visited_urls.add(url)

                                time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

                        finally:
                            # ë¸Œë¼ìš°ì € ì¢…ë£Œ
                            try: context.close() if context else None
                            except: pass
                            try: browser.close() if browser else None
                            except: pass

                except Exception as e:
                    logger.error(f"   âš ï¸ Batch Error: {e}")

                # ë°°ì¹˜ ì¢…ë£Œ í›„ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
                p = None
                gc.collect()
                time.sleep(3) # OSê°€ ìˆ¨ ëŒë¦´ ì‹œê°„ ë¶€ì—¬

        # ------------------------------------------------------------------
        # [Phase 2] ì‹ ê·œ ê²Œì„ íƒìƒ‰
        # ------------------------------------------------------------------
        if is_running:
            logger.info(f"ğŸ”­ [Phase 2] Starting Deep Discovery ...")
            base_category_path = "https://store.playstation.com/ko-kr/category/3f772501-f6f8-49b7-abac-874a88ca4897"
            search_params = "?FULL_GAME=storeDisplayClassification&GAME_BUNDLE=storeDisplayClassification&PREMIUM_EDITION=storeDisplayClassification"

            current_page = 1
            max_pages = 10
            BATCH_SIZE = CONF["restart_interval"]

            while current_page <= max_pages:
                if not is_running: break

                # Phase 2ë„ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì—”ì§„ì„ ìƒˆë¡œ ì¼¬
                try:
                    with sync_playwright() as p:
                        browser = None
                        context = None
                        try:
                            browser, context = create_browser_context(p)
                            page = setup_page(context)

                            target_list_url = f"{base_category_path}/{current_page}{search_params}"
                            logger.info(f"   ğŸ“– Scanning Page {current_page}/{max_pages}")

                            try:
                                page.goto(target_list_url, timeout=CONF['timeout'], wait_until="commit")
                                try:
                                    page.wait_for_selector("a[href*='/product/']", timeout=10000)
                                except:
                                    page.reload(timeout=CONF['timeout'], wait_until="commit")
                                    page.wait_for_selector("a[href*='/product/']", timeout=10000)
                                page.evaluate(f"window.scrollTo(0, {random.randint(800, 1200)});")
                                time.sleep(random.uniform(0.5, 1.0))
                                page.evaluate(f"window.scrollTo(0, {random.randint(3000, 4500)});")
                                time.sleep(random.uniform(1.0, 2.0))
                            except Exception as e:
                                logger.warning(f"   âš ï¸ List load failed. Skip. ({e})")
                                current_page += 1
                                continue

                            page_candidates = []
                            try:
                                links = page.locator("a[href*='/product/']").all()
                                for el in links:
                                    url = el.get_attribute("href")
                                    if url:
                                        full_url = f"https://store.playstation.com{url}" if url.startswith("/") else url
                                        if "/ko-kr/product/" in full_url and full_url not in visited_urls:
                                            if full_url not in page_candidates: page_candidates.append(full_url)
                            except: pass

                            if not page_candidates: break
                            logger.info(f"      Found {len(page_candidates)} new candidates.")

                            # ìƒì„¸ í¬ë¡¤ë§
                            for url in page_candidates:
                                if not is_running: break
                                res = crawl_detail_and_send(page, url)
                                if res:
                                    if res.get("is_delisted"):
                                        delisted_games.append(res)
                                    else:
                                        total_processed_count += 1
                                        if res.get('discountRate', 0) > 0: collected_deals.append(res)
                                visited_urls.add(url)
                                time.sleep(random.uniform(CONF["sleep_min"], CONF["sleep_max"]))

                        finally:
                            try: context.close() if context else None
                            except: pass
                            try: browser.close() if browser else None
                            except: pass

                except Exception as e:
                    logger.error(f"   ğŸ”¥ Phase 2 Error: {e}")

                gc.collect()
                time.sleep(3)
                current_page += 1

        send_discord_summary(total_processed_count, collected_deals, delisted_games)

    except Exception as e:
        logger.error(f"Critical Error: {e}")
        logger.error(traceback.format_exc())
    finally:
        with lock: is_running = False
        logger.info("ğŸ Crawler finished.")

# ==========================================
# ë‹¨ê±´ ìˆ˜ì§‘ API
# ==========================================
@app.route('/crawl/single', methods=['POST'])
def crawl_single_url():
    target_url = request.json.get('url')
    if not target_url:
        return jsonify({"error": "URL is required"}), 400

    logger.info(f"ğŸ¯ Single Crawl Request: {target_url}")

    result = None

    try:
        # âœ… with ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ Playwright ì—”ì§„ ìƒëª…ì£¼ê¸° ê´€ë¦¬
        with sync_playwright() as p:
            browser = None
            context = None

            try:
                # ë¸Œë¼ìš°ì € ìƒì„±
                browser, context = create_browser_context(p)
                page = setup_page(context)

                # í¬ë¡¤ë§ ìˆ˜í–‰
                result = crawl_detail_and_send(page, target_url, verbose=True)

            finally:
                # ğŸ§¹ ë¸Œë¼ìš°ì €ë¶€í„° ë„ê³  ë‚˜ì„œ -> pê°€ êº¼ì§€ë„ë¡ ìˆœì„œ ë³´ì¥
                logger.info("   ğŸ§¹ Cleaning up resources...")
                try: context.close() if context else None
                except: pass

                try: browser.close() if browser else None
                except: pass

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                page = None
                context = None
                browser = None
                gc.collect()

        if result:
            if result.get("is_delisted"):
                return jsonify({"status": "error", "message": "ë‹¨ì¢…ëœ ê²Œì„ì…ë‹ˆë‹¤."}), 404
            return jsonify({"status": "success", "data": result}), 200
        else:
            return jsonify({"status": "failed", "message": "Failed to parse data"}), 500

    except Exception as e:
        logger.error(f"ğŸ”¥ Single Crawl Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/run', methods=['POST'])
def trigger_crawl():
    global is_running
    with lock:
        if is_running: return jsonify({"status": "running"}), 409
        is_running = True
    thread = threading.Thread(target=run_batch_crawler_logic)
    thread.daemon = True
    thread.start()
    return jsonify({"status": "started"}), 200

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "UP", "running": is_running}), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)