import random
import os
import time
import json
import requests
import traceback
import re
import threading
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime
from flask import Flask, jsonify

import undetected_chromedriver as uc
from fake_useragent import UserAgent

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- [ì„¤ì • ë° ë¡œê¹… ì´ˆê¸°í™”] ---
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists('logs'):
    os.makedirs('logs')

log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
file_handler = RotatingFileHandler('logs/crawler.log', maxBytes=10*1024*1024, backupCount=5)
file_handler.setFormatter(log_formatter)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

logger = logging.getLogger("PS-Collector")
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

app = Flask(__name__)
session = requests.Session()
session.headers.update({'Connection': 'keep-alive'})

BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8080")
JAVA_API_URL = f"{BASE_URL}/api/v1/games/collect"
TARGET_API_URL = f"{BASE_URL}/api/v1/games/targets"
SELENIUM_URL = os.getenv("SELENIUM_URL")
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

# ë™ì‹œ ì‹¤í–‰ ë°©ì§€ ë½ (Lock)
lock = threading.Lock()
is_running = False

def get_driver():
    """ë“œë¼ì´ë²„ ì„¤ì • ë° ìƒì„± ë¡œì§ ë¶„ë¦¬"""
    # 1. ëœë¤ User-Agent ìƒì„±
    ua = UserAgent()
    random_user_agent = ua.random
    logger.info(f"ğŸ­ Generated User-Agent: {random_user_agent}")

    w = random.randint(1800, 1920)
    h = random.randint(950, 1080)
    random_window_size = f"{w},{h}"
    logger.info(f"ğŸ“ Random Window Size: {random_window_size}")

    driver = None

    # [ê³µí†µ] ì„±ëŠ¥ ìµœì í™” ì˜µì…˜
    prefs = {
        "profile.managed_default_content_settings.images": 2,       # ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨ (í•„ìˆ˜)
        "profile.default_content_setting_values.notifications": 2,  # ì•Œë¦¼ ì°¨ë‹¨
        "profile.default_content_setting_values.popups": 2,         # íŒì—… ì°¨ë‹¨
        "profile.default_content_setting_values.geolocation": 2,    # ìœ„ì¹˜ ì •ë³´ ìš”ì²­ ì°¨ë‹¨
        "disk-cache-size": 4096                                     # ë””ìŠ¤í¬ ìºì‹œ í¬ê¸° ì œí•œ
    }

    # [Case A] Docker / Selenium Grid í™˜ê²½
    if SELENIUM_URL:
        logger.info(f"ğŸŒ [Docker Mode] Connecting to Selenium Grid: {SELENIUM_URL}")
        options = webdriver.ChromeOptions()

        # Eager ëª¨ë“œ ì„¤ì •
        options.page_load_strategy = 'eager'

        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={random_window_size}")

        # ğŸš€ [ë¦¬ì†ŒìŠ¤ ì ˆì•½ ì˜µì…˜]
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage") # í˜¸ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë””ìŠ¤í¬ ì‚¬ìš©
        options.add_argument("--disable-gpu")           # GPU ì—†ìŒ ëª…ì‹œ
        options.add_argument("--no-zygote")             # í”„ë¡œì„¸ìŠ¤ í¬í¬ ìµœì†Œí™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        options.add_argument("--disable-extensions")    # í™•ì¥ í”„ë¡œê·¸ë¨ ë¹„í™œì„±í™”
        options.add_argument("--dns-prefetch-disable")  # DNS í”„ë¦¬í˜ì¹˜ ë¹„í™œì„±í™”

        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)

        # ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨ ì ìš©
        options.add_experimental_option("prefs", prefs)

        driver = webdriver.Remote(command_executor=SELENIUM_URL, options=options)

        # CDPë¥¼ í†µí•œ ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ì„¤ì • (ì¶”ê°€ ìµœì í™”)
        try:
            driver.execute_cdp_cmd("Network.setBlockedURLs", {
                "urls": ["*.png", "*.jpg", "*.gif", "*.css", "*.woff", "*.woff2", "*google-analytics*"]
            })
            driver.execute_cdp_cmd("Network.enable", {})
        except Exception as e:
            logger.warning(f"âš ï¸ CDP Optimization skipped: {e}")

    # [Case B] ë¡œì»¬ í™˜ê²½ (Undetected Chromedriver ì‚¬ìš© - ê°•ë ¥í•¨)
    else:
        logger.info("ğŸ’» [Local Mode] Starting Undetected Chrome")
        options = uc.ChromeOptions()
        options.page_load_strategy = 'eager'
        if os.getenv("HEADLESS", "false").lower() == "true":
             options.add_argument("--headless=new")

        options.add_argument(f"user-agent={random_user_agent}")
        options.add_argument(f"--window-size={random_window_size}")
        options.add_argument("--disable-popup-blocking")

        # UCëŠ” ë“œë¼ì´ë²„ ì„¤ì¹˜ë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•¨
        driver = uc.Chrome(options=options, use_subprocess=True)

    # ê³µí†µ: navigator.webdriver ìˆ¨ê¸°ê¸° (ë”ë¸” ì²´í¬)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    return driver

def fetch_update_targets():
    """Java ì„œë²„ í†µì‹  ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”"""
    try:
        res = session.get(TARGET_API_URL, timeout=30) # íƒ€ì„ì•„ì›ƒ ì¶”ê°€
        if res.status_code == 200:
            targets = res.json()
            logger.info(f"ğŸ“¥ Received {len(targets)} targets from Java Server.")
            return targets
        logger.warning(f"âš ï¸ Failed to fetch targets. Status: {res.status_code}")
        return []
    except Exception as e:
        logger.error(f"âŒ Connection Error to Java Server: {e}")
        return []

def mine_english_title(driver):
    """
    í˜ì´ì§€ ì†ŒìŠ¤ ë‚´ Script íƒœê·¸ì—ì„œ 'invariantName' (ê³µì‹ ë¶ˆë³€ ì˜ë¬¸ëª…) ì¶”ì¶œ
    Target Pattern: "invariantName":"Gran Turismoâ„¢ 7"
    """
    try:
        # 1. í˜ì´ì§€ ì†ŒìŠ¤ ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ê°€ì ¸ì˜´ (ì´ë¯¸ ë¡œë”©ëœ ìƒíƒœë¼ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì—†ìŒ - ì•ˆì „)
        src = driver.page_source

        # 2. ì •ê·œì‹ìœ¼ë¡œ "invariantName":"..." íŒ¨í„´ ê²€ìƒ‰
        # ì„¤ëª…: "invariantName" ë’¤ì— :ì´ ìˆê³ , ë”°ì˜´í‘œ(") ì•ˆì— ìˆëŠ” ê°’([^"]+)ì„ ì¡ì•„ë¼
        match = re.search(r'"invariantName"\s*:\s*"([^"]+)"', src)

        if match:
            # 3. ì°¾ì€ ê°’ (Group 1) ë¦¬í„´
            raw_title = match.group(1)

            # 4. ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ (\u0027 ë“±) ì²˜ë¦¬
            try:
                raw_title = raw_title.encode('utf-8').decode('unicode_escape')
            except: pass

            # 5. ê¹¨ì§„ ë¬¸ì ë³µêµ¬ ì‹œë„ (UTF-8 bytes -> Latin-1 interpretation fix)
            try:
                # ì–µì§€ë¡œ ë‹¤ì‹œ ì¸ì½”ë”©í–ˆë‹¤ê°€ ì œëŒ€ë¡œ ë””ì½”ë”© í•´ë³´ê¸°
                raw_title = raw_title.encode('latin1').decode('utf-8')
            except: pass

            # 6. íŠ¹ìˆ˜ë¬¸ì ì¹˜í™˜ (IGDB ê²€ìƒ‰ì„ ìœ„í•´ ì•„ì˜ˆ í‘œì¤€ ë¬¸ìë¡œ ë³€ê²½)
            # ìŠ¤ë§ˆíŠ¸ ë”°ì˜´í‘œ(â€™) -> ì¼ë°˜ ë”°ì˜´í‘œ(')
            raw_title = raw_title.replace("â€™", "'").replace("â€˜", "'")
            # TM(â„¢), R(Â®) -> ì‚­ì œ (ë¶ˆí•„ìš” ë¬¸ì)
            raw_title = re.sub(r'[â„¢Â®Ã¢Â¢]', '', raw_title)

            logger.info(f"   ğŸ’ Mined Invariant Title: {raw_title}")
            return raw_title.strip()

        return None

    except Exception as e:
        logger.warning(f"   âš ï¸ Mining failed: {e}")

    return None

def send_discord_summary(total_scanned, deals_list):
    """í¬ë¡¤ë§ ì¢…ë£Œ í›„ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ë””ìŠ¤ì½”ë“œë¡œ ì „ì†¡"""
    if not DISCORD_WEBHOOK_URL:
        return

    try:
        total_deals = len(deals_list)
        if total_deals == 0:
            logger.info("ğŸ“­ No deals found today. Skipping Discord report.")
            return

        # í• ì¸ìœ¨ ë†’ì€ ìˆœ ì •ë ¬ ë° ìƒìœ„ 5ê°œ ì¶”ì¶œ
        sorted_deals = sorted(deals_list, key=lambda x: x['discountRate'], reverse=True)
        top_5 = sorted_deals[:5]

        # [í—¤ë”] í†µê³„ ìš”ì•½
        message = f"## ğŸ“¢ [PS-Tracker] ì¼ì¼ ìˆ˜ì§‘ ë¦¬í¬íŠ¸\n"
        message += f"**ğŸ—“ï¸ ë‚ ì§œ:** {datetime.now().strftime('%Y-%m-%d')}\n"
        message += f"**ğŸ“Š í†µê³„:** ì´ `{total_scanned}`ê°œ ìŠ¤ìº” / **`{total_deals}`**ê°œ í• ì¸ ê°ì§€! ğŸ”¥\n"
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

        # [ë©”ì¸] Top 5 ê³¨ë“  ë”œ ìƒì„¸ ë¦¬ìŠ¤íŒ…
        message += "**ğŸ† ì˜¤ëŠ˜ì˜ Top 5 í• ì¸**\n"
        for i, game in enumerate(top_5, 1):
            sale_price = "{:,}".format(game['currentPrice'])
            plat_list = game.get('platforms', [])
            plat_str = f" | `{'/'.join(plat_list)}`" if plat_list else ""

            # í•œ ê²Œì„ì”© ë¸”ë¡í™”í•˜ì—¬ ì¶œë ¥
            message += f"{i}ï¸âƒ£ **[{game['discountRate']}%] {game['title']}**\n"
            message += f"ã€€ ğŸ’° **â‚©{sale_price}**{plat_str}\n"
            message += f"ã€€ â³ ~{game['saleEndDate'] or 'ìƒì‹œ ì¢…ë£Œ'}\n"

            # ê°€ë…ì„±ì„ ìœ„í•œ êµ¬ë¶„ì„  ì¶”ê°€ (ë§ˆì§€ë§‰ í•­ëª© ì œì™¸)
            if i < len(top_5):
                message += "â”€â”€â”€\n"

        # [í‘¸í„°] í•˜ë‹¨ ì •ë³´ ë° ë§í¬
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        if total_deals > 5:
            message += f"ì™¸ **{total_deals - 5}**ê°œì˜ í• ì¸ì´ ë” ìˆìŠµë‹ˆë‹¤!\n"

        message += "\n[ğŸ”— ì‹¤ì‹œê°„ ìµœì €ê°€ í™•ì¸í•˜ê¸°](https://ps-signal.com)"

        # ë””ìŠ¤ì½”ë“œ ì „ì†¡
        requests.post(DISCORD_WEBHOOK_URL, json={"content": message})
        logger.info("ğŸ”” Polished Discord Summary Report sent!")

    except Exception as e:
        logger.error(f"âŒ Failed to send Discord summary: {e}")

def run_batch_crawler_logic():
    global is_running
    logger.info("ğŸš€ [Crawler] Batch job started - Safety Optimized Mode")

    driver = None

    total_processed_count = 0
    collected_deals = []

    try:
        driver = get_driver()
        wait = WebDriverWait(driver, 10)
        visited_urls = set()

        # [Phase 1] ê¸°ì¡´ íƒ€ê²Ÿ ê°±ì‹ 
        targets = fetch_update_targets()
        if targets:
            logger.info(f"ğŸ”„ [Phase 1] Updating {len(targets)} tracked games...")

            for i, url in enumerate(targets):
                if not is_running: break

                # 40ê°œë§ˆë‹¤ ë¸Œë¼ìš°ì €ë¥¼ ê»ë‹¤ ì¼œì„œ ëˆ„ìˆ˜ëœ ë©”ëª¨ë¦¬ë¥¼ ê°•ì œë¡œ ë°˜í™˜
                if i > 0 and i % 40 == 0:
                    logger.info(f"â™»ï¸ [Phase 1] Memory Cleanup at item {i}... Restarting Driver.")
                    try:
                        driver.quit()
                    except: pass
                    time.sleep(5)
                    driver = get_driver()
                    wait = WebDriverWait(driver, 10)

                # í¬ë¡¤ë§ ìˆ˜í–‰
                deal_info = crawl_detail_and_send(driver, wait, url)

                if deal_info:
                    total_processed_count += 1
                    if deal_info.get('discountRate', 0) > 0:
                        collected_deals.append(deal_info)

                visited_urls.add(url)

                time.sleep(random.uniform(2.5, 4.0))

        # [Phase 2] ì‹ ê·œ íƒìƒ‰ (í˜ì´ì§€ë„¤ì´ì…˜)
        if is_running:
            logger.info(f"ğŸ”­ [Phase 2] Starting Deep Discovery (Max 300 Pages)...")
            base_category_path = "https://store.playstation.com/ko-kr/category/3f772501-f6f8-49b7-abac-874a88ca4897"
            search_params = "?FULL_GAME=storeDisplayClassification&GAME_BUNDLE=storeDisplayClassification&PREMIUM_EDITION=storeDisplayClassification"

            current_page = 1
            max_pages = 15

            while current_page <= max_pages:
                if not is_running: break

                # [ë©”ëª¨ë¦¬ ê´€ë¦¬] 2í˜ì´ì§€ë§ˆë‹¤ ë“œë¼ì´ë²„ ì¬ì‹œì‘
                if current_page > 1 and current_page % 2 == 0:
                    logger.info("â™»ï¸ [Maintenance] Restarting driver to prevent memory leak...")
                    try:
                        driver.quit()
                    except: pass
                    time.sleep(5)
                    driver = get_driver()
                    wait = WebDriverWait(driver, 10)

                target_list_url = f"{base_category_path}/{current_page}{search_params}"
                logger.info(f"   ğŸ“– Scanning Page {current_page}/{max_pages}")

                try:
                    driver.get(target_list_url)

                    # ìŠ¤í¬ë¡¤ ë¡œì§
                    try:
                        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='/product/']")))
                    except TimeoutException:
                        logger.warning(f"   âš ï¸ Page load timeout. Retrying...")
                        driver.refresh()
                        time.sleep(3)

                    # ìŠ¤í¬ë¡¤ ë¡œì§
                    driver.execute_script(f"window.scrollTo(0, {random.randint(800, 1200)});")
                    time.sleep(random.uniform(0.5, 1.5))
                    driver.execute_script(f"window.scrollTo(0, {random.randint(3000, 4500)});")
                    time.sleep(random.uniform(1.5, 2.5))

                except Exception as e:
                    logger.warning(f"âš ï¸ Page Load Error on {current_page}: {e}")

                # ë§í¬ ìˆ˜ì§‘
                page_candidates = []
                try:
                    link_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/product/']")
                    for el in link_elements:
                        url = el.get_attribute("href")
                        if url and "/ko-kr/product/" in url and url not in visited_urls:
                            if url not in page_candidates:
                                page_candidates.append(url)
                except: pass

                # ì¢…ë£Œ ì¡°ê±´
                if not page_candidates:
                    logger.info(f"ğŸ›‘ No new games found on page {current_page}. Finishing Phase 2.")
                    break

                logger.info(f"      Found {len(page_candidates)} new candidates.")

                # ìƒì„¸ í¬ë¡¤ë§
                for url in page_candidates:
                    if not is_running: break
                    deal_info = crawl_detail_and_send(driver, wait, url)

                    if deal_info:
                        total_processed_count += 1
                        if deal_info.get('discountRate', 0) > 0:
                            collected_deals.append(deal_info)

                    visited_urls.add(url)
                    time.sleep(random.uniform(2.5, 4.0))

                current_page += 1
                time.sleep(random.uniform(3.0, 5.0))

            logger.info(f"âœ… Batch job finished. Total processed: {len(visited_urls)} games.")

            # ë””ìŠ¤ì½”ë“œ ìš”ì•½ ì „ì†¡
            send_discord_summary(total_processed_count, collected_deals)
    except Exception as e:
        logger.error(f"ğŸ”¥ Critical Crawler Error: {e}")
        logger.error(traceback.format_exc())
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("ğŸ”Œ Driver closed.")
            except: pass

        with lock:
            is_running = False

def crawl_detail_and_send(driver, wait, target_url):
    try:
        driver.get(target_url)

        # 1. ì œëª© ë¡œë”©
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "[data-qa='mfe-game-title#name']")))
        except TimeoutException:
            logger.warning(f"â³ Timeout loading title: {target_url}")
            return

        # 2. ê°€ê²© ì»¨í…Œì´ë„ˆ ëŒ€ê¸° (ì—†ìœ¼ë©´ ë¬´ë£Œ ê²Œì„ì´ê±°ë‚˜ ë¡œë”© ì‹¤íŒ¨)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "[data-qa^='mfeCtaMain#offer']")))
        except:
            # ê°€ê²©ì´ ì—†ëŠ” ê²½ìš°(ì˜ˆ: ì¶œì‹œ ì˜ˆì •ì‘)ë„ ìˆìœ¼ë¯€ë¡œ ë¡œê·¸ë§Œ ì°ê³  ì§„í–‰
            logger.info("   â„¹ï¸ No price container found (Might be free or unreleased)")
            pass

        # ì˜ë¬¸ íƒ€ì´í‹€ ëª… ì±„êµ´
        english_title = mine_english_title(driver)

        # 3. ì œëª© ì¶”ì¶œ
        try:
            title = driver.find_element(By.CSS_SELECTOR, "[data-qa='mfe-game-title#name']").text.strip()
            logger.info(f"   ğŸ“– Title: {title}")
        except:
            title = "Unknown Title"
            logger.warning("   âš ï¸ Failed to extract title")

        # 4. ì¥ë¥´ ì¶”ì¶œ
        genre_ids = ""
        try:
            genre_element = driver.find_element(By.CSS_SELECTOR, "[data-qa='gameInfo#releaseInformation#genre-value']")
            genre_ids = genre_element.text
        except: pass

        # 5. í”Œë«í¼ ì¶”ì¶œ
        platform_set = set()
        try:
            tag_elements = driver.find_elements(By.CSS_SELECTOR, "[data-qa^='mfe-game-title#productTag']")
            for el in tag_elements:
                raw_text = el.get_attribute("textContent").strip().upper()
                if "PS5" in raw_text: platform_set.add("PS5")
                if "PS4" in raw_text: platform_set.add("PS4")
                if "VR2" in raw_text: platform_set.add("PS_VR2")
                elif "VR" in raw_text: platform_set.add("PS_VR")
            platforms = list(platform_set)
        except Exception as e:
            platforms = []

        # 6.. ê°€ê²© ì¶”ì¶œ
        best_price = float('inf')
        best_offer_data = None    # ìµœì €ê°€ì¼ ë•Œì˜ ì„¸ë¶€ ì •ë³´(ì›ê°€, í• ì¸ìœ¨, Plusì—¬ë¶€ ë“±)
        found_valid_offer = False

        # DOM ì•ˆì •í™” ëŒ€ê¸°
        time.sleep(1.0)

        # ìµœëŒ€ 2ë²ˆ ì‹œë„ (DOM ë Œë”ë§ ì§€ì—° ëŒ€ë¹„)
        for attempt in range(2):
            if found_valid_offer: break
            if attempt > 0: time.sleep(1)

            # ëª¨ë“  ì˜¤í¼(offer0 ~ offer2)ë¥¼ ë‹¤ í™•ì¸í•´ì„œ ê°€ì¥ ì‹¼ ê°€ê²©ì„ ì„ íƒ
            for i in range(3):
                try:
                    # í•´ë‹¹ ìˆœë²ˆ(i)ì˜ ê°€ê²© ë°•ìŠ¤ ì „ì²´ë¥¼ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
                    offer_selector = f"[data-qa='mfeCtaMain#offer{i}']"
                    try:
                        offer_container = driver.find_element(By.CSS_SELECTOR, offer_selector)
                    except:
                        continue # í•´ë‹¹ ë²ˆí˜¸ì˜ ì˜¤í¼ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒìœ¼ë¡œ

                    # 6-1. ê°€ê²© íŒŒì‹± (textContent ì‚¬ìš©ìœ¼ë¡œ í™”ë©´ ê°€ë¦¼ ë¬¸ì œ í•´ê²°)
                    try:
                        price_selector = f"[data-qa='mfeCtaMain#offer{i}#finalPrice']"
                        price_elem = offer_container.find_element(By.CSS_SELECTOR, price_selector)
                        raw_price = price_elem.get_attribute("textContent").strip()

                        # "í¬í•¨" ë“± ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ê±´ë„ˆë›°ê¸° (PS Plus ë¬´ë£Œ ì˜¤í¼ íšŒí”¼ìš©)
                        clean_price_text = re.sub(r'[^0-9]', '', raw_price)
                        if not clean_price_text: continue

                        current_price = int(clean_price_text)
                        if current_price == 0: continue
                    except: continue

                    # 6-2. PS Plus ì—¬ë¶€ íŒŒì‹±
                    is_plus = False

                    # [Check 1] ë…¸ë€ìƒ‰ í…ìŠ¤íŠ¸ í´ë˜ìŠ¤ (ê°€ì¥ í™•ì‹¤)
                    # HTML: <span class="psw-c-t-ps-plus ...">PlayStation Plusë¡œ ...</span>
                    try:
                        if offer_container.find_elements(By.CSS_SELECTOR, ".psw-c-t-ps-plus"):
                            is_plus = True
                    except: pass

                    # [Check 2] ì•„ì´ì½˜ (serviceIcon#ps-plus)
                    # HTML: <span data-qa="mfeCtaMain#offer0#serviceIcon#ps-plus" ...>
                    if not is_plus:
                        try:
                            if offer_container.find_elements(By.CSS_SELECTOR, "[data-qa*='serviceIcon#ps-plus']"):
                                is_plus = True
                        except: pass

                    # [Check 3] í…ìŠ¤íŠ¸ ë³´ì¡° í™•ì¸
                    if not is_plus:
                        try:
                            container_text = offer_container.text
                            if "Plus" in container_text and ("í• ì¸" in container_text or "ì ˆì•½" in container_text):
                                is_plus = True
                        except: pass

                    # 6-3 ì›ê°€ íŒŒì‹±
                    original_price = current_price # ê¸°ë³¸ê°’
                    found_original = False

                    # ì „ëµ A: ëª…ì‹œì  íƒœê·¸ (data-qa)
                    try:
                        orig_elem = offer_container.find_element(By.CSS_SELECTOR, f"[data-qa='mfeCtaMain#offer{i}#originalPrice']")
                        raw_orig = orig_elem.get_attribute("textContent").strip()
                        parsed_orig = int(re.sub(r'[^0-9]', '', raw_orig))
                        if parsed_orig > current_price:
                            original_price = parsed_orig
                            found_original = True
                    except: pass

                    # ì „ëµ B: CSS í´ë˜ìŠ¤ (psw-t-strike) - ì œê³µëœ HTMLì—ì„œ í™•ì¸ëœ í´ë˜ìŠ¤ëª…!
                    if not found_original:
                        try:
                            # psw-t-strike: ì†Œë‹ˆ ìŠ¤í† ì–´ì˜ 'ì·¨ì†Œì„ ' ìŠ¤íƒ€ì¼ í´ë˜ìŠ¤
                            strikethrough_elems = offer_container.find_elements(By.CSS_SELECTOR, ".psw-t-strike")
                            for elem in strikethrough_elems:
                                raw_orig = elem.get_attribute("textContent").strip()
                                clean_orig = re.sub(r'[^0-9]', '', raw_orig)
                                if clean_orig:
                                    parsed_price = int(clean_orig)
                                    # ì›ê°€ê°€ í˜„ì¬ê°€ë³´ë‹¤ ì»¤ì•¼ ìœ íš¨
                                    if parsed_price > current_price:
                                        original_price = parsed_price
                                        found_original = True
                                        break
                        except: pass

                    # í• ì¸ìœ¨ ê³„ì‚°
                    discount_rate = 0
                    if original_price > current_price:
                        discount_rate = int(round(((original_price - current_price) / original_price) * 100))

                    # 6-4 ì¢…ë£Œì¼ íŒŒì‹±
                    sale_end_date = None
                    try:
                        date_elem = offer_container.find_element(By.CSS_SELECTOR, f"[data-qa='mfeCtaMain#offer{i}#discountDescriptor']")
                        raw_date_text = date_elem.get_attribute("textContent")

                        # HTML ì˜ˆì‹œ: "2025/12/22 ì˜¤í›„ 11:59..." -> YYYY/MM/DD ì¶”ì¶œ
                        match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', raw_date_text)
                        if match:
                            sale_end_date = f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
                    except: pass

                    # 6-5 ìµœì €ê°€ ë¹„êµ
                    if current_price < best_price:
                        best_price = current_price
                        best_offer_data = {
                            "originalPrice": original_price,
                            "currentPrice": current_price,
                            "discountRate": discount_rate,
                            "saleEndDate": sale_end_date,
                            "isPlusExclusive": is_plus
                        }
                        found_valid_offer = True

                except Exception:
                    continue

        # [ë°ì´í„° ì—†ìŒ ì²˜ë¦¬]
        if not found_valid_offer or best_offer_data is None:
            logger.warning(f"ğŸš« Skip: Valid price not found for {title}")
            return

        # 5. ì „ì†¡
        image_url = ""
        try:
            # ëª¨ë“  JSON ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ë¥¼ ê°€ì ¸ì˜´.
            scripts = driver.find_elements(By.CSS_SELECTOR, "script[type='application/json']")
            for script in scripts:
                content = script.get_attribute("innerHTML")
                # "media" í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ì–´ì„œ ì†ë„ í–¥ìƒ
                if "media" not in content or "url" not in content:
                    continue

                try:
                    data = json.loads(content)
                    # cache ê°ì²´ ë‚´ë¶€ ìˆœíšŒ (Concept:1234, Product:1234 ë“± ë™ì  í‚¤ ëŒ€ì‘)
                    cache = data.get("cache", {})
                    for key, val in cache.items():
                        # personalizedMeta -> media êµ¬ì¡° í™•ì¸
                        if "personalizedMeta" in val and "media" in val["personalizedMeta"]:
                            media_list = val["personalizedMeta"]["media"]
                            # ìš°ì„ ìˆœìœ„: MASTER > GAMEHUB_COVER_ART > ì•„ë¬´ê±°ë‚˜
                            for media in media_list:
                                if media.get("role") == "MASTER":
                                    image_url = media.get("url")
                                    break
                                if media.get("role") == "GAMEHUB_COVER_ART" and not image_url:
                                    image_url = media.get("url")

                            if image_url: break # ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
                    if image_url: break
                except: pass

            if image_url:
                logger.info(f"   ğŸ“¸ Image found via JSON Script (Master/Cover)")
        except Exception as e:
            logger.warning(f"   âš ï¸ JSON extraction error: {e}")

        # [ì „ëµ 2] Meta Tag ë°±ì—… (og:image)
        # ë§Œì•½ JSON êµ¬ì¡°ê°€ ë°”ë€Œì—ˆì„ ë•Œë¥¼ ëŒ€ë¹„í•œ ì•ˆì „ì¥ì¹˜
        if not image_url:
            try:
                meta_img = driver.find_element(By.CSS_SELECTOR, "meta[property='og:image']")
                image_url = meta_img.get_attribute("content").split("?")[0]
                logger.info(f"   ğŸ“¸ Image found via Meta Tag")
            except: pass

        ps_store_id = target_url.split("/")[-1].split("?")[0]

        payload = {
            "psStoreId": ps_store_id,
            "title": title,
            "englishTitle": english_title,
            "publisher": "Batch Crawler",
            "imageUrl": image_url,
            "description": "Full Data Crawler",
            "originalPrice": best_offer_data["originalPrice"],
            "currentPrice": best_offer_data["currentPrice"],
            "discountRate": best_offer_data["discountRate"],
            "saleEndDate": best_offer_data["saleEndDate"],
            "isPlusExclusive": best_offer_data["isPlusExclusive"],
            "genreIds": genre_ids,
            "platforms": platforms
        }

        send_data_to_server(payload, title)

        return payload
    except Exception as e:
        logger.error(f"   âš ï¸ Fatal Error processing {target_url}: {e}")
        return None

def send_data_to_server(payload, title):
    try:
        res = session.post(JAVA_API_URL, json=payload, timeout=30)
        if res.status_code == 200:
            logger.info(f"   ğŸ“¤ Sent: {title} ({payload['currentPrice']} KRW)")
        else:
            logger.error(f"   ğŸ’¥ Server Error ({res.status_code}): {title}")
    except requests.exceptions.Timeout:
        logger.error(f"   â³ Timeout Error: Server took too long to respond for {title}")
    except Exception as e:
        logger.error(f"   ğŸ’¥ Network Error sending {title}: {e}")

# --- [API ì—”ë“œí¬ì¸íŠ¸] ---
@app.route('/run', methods=['POST'])
def trigger_crawl():
    global is_running

    # Thread-safe Lock ì‚¬ìš©
    with lock:
        if is_running:
            return jsonify({"status": "error", "message": "Crawler is already running"}), 409
        is_running = True

    # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
    thread = threading.Thread(target=run_batch_crawler_logic)
    thread.daemon = True # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ê°™ì´ ì¢…ë£Œë˜ë„ë¡ ì„¤ì •
    thread.start()

    return jsonify({"status": "success", "message": "Crawler started in background"}), 200

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "UP", "crawler_running": is_running}), 200

if __name__ == "__main__":
    logger.info("ğŸ‘‚ [Collector] Server starting on port 5000...")
    app.run(host="0.0.0.0", port=5000)