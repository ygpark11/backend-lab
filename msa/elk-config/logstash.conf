# Logstash 파이프라인 설정

input {
  # (1) Input: /var/log/msa-logs/ 폴더에 있는 모든 .log 파일을 감시
  file {
    path => "/var/log/msa-logs/*.log"
    start_position => "beginning" # 처음부터 읽기
    sincedb_path => "/dev/null"   # (개발용) 매번 재시작 시 처음부터 다시 읽기
  }
}

filter {
  # Grok 필터를 사용하여 'message' 필드를 파싱
  grok {
    # 'message' 필드의 내용이 "INFO [order-service,abc,123] ... " 패턴과 일치하는지 확인
    match => { "message" => "%{LOGLEVEL:log.level} \[%{DATA:service.name},%{DATA:trace.id},%{DATA:span.id}\] %{GREEDYDATA:log.message}" }
  }

  # (선택) 파싱에 성공한 원본 'message' 필드는 삭제 (Kibana에서 깔끔하게 보임)
  mutate {
    remove_field => ["message"]
  }
}

output {
  # (3) Output: 가공된 데이터를 Elasticsearch로 전송
  elasticsearch {
    hosts => ["http://elasticsearch:9200"] # docker-compose의 서비스 이름
    index => "msa-logs-%{+YYYY.MM.dd}"     # 날짜별로 인덱스(테이블) 생성
  }

  # (4) 디버깅용: 가공된 데이터를 Logstash 콘솔에도 출력
  stdout { codec => rubydebug }
}